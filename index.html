<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>Stlite App</title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@stlite/mountable@0.64.2/build/stlite.css"
    />
  </head>
  <body>
    <div id="root"></div>
    <script src="https://cdn.jsdelivr.net/npm/@stlite/mountable@0.64.2/build/stlite.js"></script>
    <script>
           stlite.mount(
            {
                requirements: ["pandas", "matplotlib"],
                entrypoint: "main.py",
                files: {
                    ".streamlit/config.toml": `
[theme]
base="dark"
      `,
                    "main.py": `import pandas as pd
import os
import io
from concurrent.futures import ThreadPoolExecutor, as_completed
from LCR import LCR
from NSFR import NSFR
from AER import AER
from ALMM import ALMM
from QIS import QIS
from datetime import datetime
import streamlit as st
import shutil
import tempfile
import zipfile
from openpyxl import load_workbook
from io import BytesIO

Entity_List = ['BANCO SOCIETE GENERALE BRASIL SA','BPCE LEASE','FRAER LEASING SPA','FRANFINANCE','FRANFINANCE LOCATION','GEFA BANK GMBH','GERMAN NEWCO','GERMAN NEWCO','MILLA','PHILIPS MEDICAL CAPITAL FRANCE','SG EQUIPMENT FINANCE BENELUX BV','SG EQUIPMENT FINANCE CZECH REPUBLIC','SG EQUIPMENT FINANCE GMBH','SG EQUIPMENT FINANCE IBERIA','SG EQUIPMENT FINANCE ITALY SPA','SG EQUIPMENT FINANCE SCHWEIZ AG','SG EQUIPMENT FINANCE USA CORP','SG EQUIPMENT LEASING POLSKA SP ZO','SG EQUIPMENT LEASING POLSKA SP ZO','SG LEASING SPA','SGEF SA','SGEF SA ARRENDAMENTO MERCANTIL','SOCIETE GENERALE EQUIPMENT FINANCE Brazil','SOCIETE GENERALE EQUIPMENT FINANCE UK','SOCIETE GENERALE LEASING AND RENTING China']
expected_columns = [
    "D_CA", "D_DP", "D_ZTFTR", "D_PE", "D_RU", "D_ORU", "D_AC", "D_FL", "D_AU", 
    "D_T1", "D_T2", "D_CU", "D_TO", "D_GO", "D_LE", "D_NU", "D_DEST", "D_ZONE", 
    "D_MONNAIE", "D_ENTITE", "D_RESTIT", "D_TYPCLI", "D_SURFI", "D_MU", "D_PMU", 
    "D_ACTIVITE", "D_ANALYSIS", "D_PDT", "P_AMOUNT", "P_COMMENT"
]

def preprocess_all_data(data_path, ref_entite_path, ref_transfo_path, ref_lcr_path, ref_adf_lcr_path,
                        input_excel_path, run_timestamp, export_type, currency="ALL"):
    """
    Prétraitement des données pour tous les types d'export (ALL, BILAN, CONSO, GRAN).
    """
    try:
        data_import = pd.read_excel(data_path, engine="openpyxl")
    except Exception as e:
        raise ValueError(f"Erreur lors du chargement des données principales : {e}")

    # Vérifier les colonnes essentielles
    required_columns = ["D_CU", "D_T1", "D_ENTITE", "D_PE"]
    missing_columns = [col for col in required_columns if col not in data_import.columns]
    if missing_columns:
        raise ValueError(f"Les colonnes suivantes sont manquantes dans les données : {', '.join(missing_columns)}")

    # Initialiser le processeur LCR
    lcr_processor = LCR(
        data_import=data_import,
        ref_entite_path=ref_entite_path,
        ref_transfo_path=ref_transfo_path,
        ref_lcr_path=ref_lcr_path,
        ref_adf_lcr_path=ref_adf_lcr_path,
        input_excel_path=input_excel_path,
        run_timestamp=run_timestamp,
        export_type=export_type
    )

    # Prétraitement des données
    if export_type == "GRAN":
        # Vérification avant filtrage
        print(f"Valeurs uniques dans D_CU : {data_import['D_CU'].unique()}")

        # Filtrage des données par devise
        if currency == "ALL":
            filtered_data = data_import  # Prendre toutes les devises
        else:
            filtered_data = data_import[data_import["D_CU"] == currency]

        print("Filtrage réussi pour GRAN. Données disponibles :")
        print(filtered_data.head())  # Log des premières lignes pour vérification

        return {"filtered_data": filtered_data}
    else:
        # Prétraitement standard pour ALL, BILAN, et CONSO
        preprocessed_data = lcr_processor.preprocess_data(export_type=export_type, currency=currency)

        if isinstance(preprocessed_data, dict):
            return preprocessed_data
        else:
            raise ValueError("Le prétraitement des données a échoué pour les exports standard.")

def process_aer(preprocessed_data,
                data_path, ref_entite_path, ref_transfo_path, ref_aer_path, ref_adf_aer_path,
                input_excel_path, run_timestamp, export_type, zip_buffer,
                entity=None, currency=None, indicator="ALL"):
    """
    Processus pour traiter les données AER avec gestion spécifique des exports dans un ZIP,
    incluant la transition des données vers un fichier template.
    """
    base_folder = f"RUN_{run_timestamp}_{export_type}"  # Dossier racine dans le ZIP

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        if export_type == "GRAN":

            if not entity or not currency:
                raise ValueError("Pour un export de type GRAN, une entité et une devise spécifiques doivent être fournies.")

            print(f"Traitement GRAN pour l'entité '{entity}' et la devise '{currency}'...")

            # Filtrer les données pour GRAN
            if isinstance(preprocessed_data, pd.DataFrame):
                if "D_CU" not in preprocessed_data.columns:
                    raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                if currency == "ALL":
                    filtered_data = preprocessed_data
                else:
                    filtered_data = preprocessed_data[preprocessed_data["D_CU"] == currency]
            elif isinstance(preprocessed_data, dict):
                if "filtered_data" in preprocessed_data:
                    filtered_data = preprocessed_data["filtered_data"]
                    if "D_CU" not in filtered_data.columns:
                        raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                    if currency == "ALL":
                        filtered_data = filtered_data
                    else:
                        filtered_data = filtered_data[filtered_data["D_CU"] == currency]
                else:
                    raise ValueError("La clé 'filtered_data' est absente dans preprocessed_data.")
            else:
                raise TypeError("preprocessed_data doit être un DataFrame ou un dictionnaire.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour la devise '{currency}' dans l'export GRAN.")

            # Étape 2 : Filtrer par indicateur
            if indicator == "BILAN":
                filtered_data = filtered_data[filtered_data["D_T1"] == "INTER"]
            elif indicator == "CONSO":
                filtered_data = filtered_data[filtered_data["D_T1"] != "INTER"]
            elif indicator == "ALL":
                pass  # Ne rien filtrer
            else:
                raise ValueError("Indicateur non pris en charge. Choisissez parmi ALL, BILAN, ou CONSO.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour l'indicateur '{indicator}'.")

            # Initialiser la classe AER
            aer_processor = AER(
                data_import=filtered_data,
                ref_entite_path=ref_entite_path,
                ref_transfo_path=ref_transfo_path,
                ref_aer_path=ref_aer_path,
                ref_adf_aer_path=ref_adf_aer_path,
                run_timestamp=run_timestamp,
                export_type=export_type,
            )

            # Appliquer les transformations
            result_after_entite = aer_processor.filter_and_join_ref_entite(filtered_data)
            result_after_transfo = aer_processor.join_with_ref_transfo(result_after_entite)
            result_with_aer = aer_processor.join_with_ref_aer(result_after_transfo)
            grouped_result = aer_processor.group_and_join_ref_adf_aer(result_with_aer)
            final_result = aer_processor.add_adjusted_amount(grouped_result)

            # Filtrer par entité
            final_result = final_result[final_result["Ref_Entite.entité"] == entity]

            # Transition vers le fichier template
            buffer = apply_to_template(final_result, input_excel_path)

            # Ajouter au ZIP
            folder_path = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
            file_name = f"{folder_path}/AER_GRAN_{currency}_{entity}.xlsx"
            zipf.writestr(file_name, buffer.getvalue())

        else:  # Cas ALL, BILAN, CONSO
            for currency, file_path in preprocessed_data.items():
                if not os.path.exists(file_path):
                    print(f"Le fichier {file_path} n'existe pas. Aucun traitement pour cette devise.")
                    continue

                try:
                    data_import_filtered = pd.read_excel(file_path, engine="openpyxl")
                except Exception as e:
                    print(f"Erreur lors de la lecture du fichier {file_path}: {e}")
                    continue

                if data_import_filtered.empty:
                    continue

                print(f"Traitement de la devise : {currency}")

                # Initialiser la classe AER
                aer_processor = AER(
                    data_import=data_import_filtered,
                    ref_entite_path=ref_entite_path,
                    ref_transfo_path=ref_transfo_path,
                    ref_aer_path=ref_aer_path,
                    ref_adf_aer_path=ref_adf_aer_path,
                    run_timestamp=run_timestamp,
                    export_type=export_type,
                )

                # Appliquer les transformations
                result_after_entite = aer_processor.filter_and_join_ref_entite(data_import_filtered)
                result_after_transfo = aer_processor.join_with_ref_transfo(result_after_entite)
                result_with_aer = aer_processor.join_with_ref_aer(result_after_transfo)
                grouped_result = aer_processor.group_and_join_ref_adf_aer(result_with_aer)
                final_result = aer_processor.add_adjusted_amount(grouped_result)

                # Transition vers le fichier template
                buffer = apply_to_template(final_result, input_excel_path)

                # Ajouter au ZIP
                folder_path_global = f"{base_folder}/{currency}/Reports_all_entities"
                file_name_global = f"{folder_path_global}/AER_{export_type}_{currency}_All_Entities.xlsx"
                zipf.writestr(file_name_global, buffer.getvalue())
                
                # Ne générer que les rapports globaux si export_type == 'ALL'
                if export_type == 'ALL':
                    continue
                else:
                    # Sauvegarder les fichiers par entité
                    for entity in final_result["Ref_Entite.entité"].unique():
                        entity_data = final_result[final_result["Ref_Entite.entité"] == entity]
                        if entity_data.empty:
                            continue
                        buffer_entity = apply_to_template(entity_data, input_excel_path)
                        folder_path_entity = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
                        file_name_entity = f"{folder_path_entity}/AER_{export_type}_{currency}_{entity}.xlsx"
                        zipf.writestr(file_name_entity, buffer_entity.getvalue())

    print("Tous les fichiers AER ont été ajoutés au ZIP.")


def process_qis(
    preprocessed_data,
    data_path,
    ref_entite_path,
    ref_transfo_path,
    ref_qis_path,
    ref_adf_qis_path,
    ref_dzone_qis_path,
    input_excel_path,
    run_timestamp,
    export_type,
    zip_buffer,
    entity=None,
    currency=None,
    indicator="ALL"
):
    """
    Processus pour traiter les données QIS avec gestion spécifique des exports dans un ZIP,
    incluant la transition des données vers un fichier template.
    """
    base_folder = f"RUN_{run_timestamp}_{export_type}"  # Dossier racine dans le ZIP

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        if export_type == "GRAN":
            if not entity or not currency:
                raise ValueError("Pour un export de type GRAN, une entité et une devise spécifiques doivent être fournies.")

            print(f"Traitement GRAN pour l'entité '{entity}' et la devise '{currency}'...")

            # Filtrer les données pour GRAN
            if isinstance(preprocessed_data, pd.DataFrame):
                if "D_CU" not in preprocessed_data.columns:
                    raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                if currency == "ALL":
                    filtered_data = preprocessed_data
                else:
                    filtered_data = preprocessed_data[preprocessed_data["D_CU"] == currency]
            else:
                raise TypeError("preprocessed_data doit être un DataFrame pour un export de type GRAN.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour la devise '{currency}' dans l'export GRAN.")

            # Initialiser la classe QIS
            qis_processor = QIS(
                data_import=filtered_data,
                ref_entite_path=ref_entite_path,
                ref_transfo_path=ref_transfo_path,
                ref_qis_path=ref_qis_path,
                ref_adf_qis_path=ref_adf_qis_path,
                ref_dzone_qis_path=ref_dzone_qis_path,
                run_timestamp=run_timestamp,
                export_type=export_type,
            )

            # Appliquer les transformations
            result_after_entite = qis_processor.filter_and_join_ref_entite(filtered_data)
            result_after_transfo = qis_processor.join_with_ref_transfo(result_after_entite)
            result_with_qis = qis_processor.join_with_ref_qis(result_after_transfo)
            grouped_result = qis_processor.group_and_join_ref_adf_qis(result_with_qis)
            final_result = qis_processor.add_adjusted_amount(grouped_result)

            # Filtrer par entité
            final_result = final_result[final_result["Ref_Entite.entité"] == entity]

            # Transition vers le fichier template
            buffer = apply_to_template(final_result, input_excel_path)

            # Ajouter au ZIP
            folder_path = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
            file_name = f"{folder_path}/QIS_GRAN_{currency}_{entity}.xlsx"
            zipf.writestr(file_name, buffer.getvalue())

        else:  # Cas ALL, BILAN, CONSO
            for currency, file_path in preprocessed_data.items():
                if not os.path.exists(file_path):
                    print(f"Le fichier {file_path} n'existe pas. Aucun traitement pour cette devise.")
                    continue

                try:
                    data_import_filtered = pd.read_excel(file_path, engine="openpyxl")
                except Exception as e:
                    print(f"Erreur lors de la lecture du fichier {file_path}: {e}")
                    continue

                if data_import_filtered.empty:
                    continue

                print(f"Traitement de la devise : {currency}")

                # Initialiser la classe QIS
                qis_processor = QIS(
                    data_import=data_import_filtered,
                    ref_entite_path=ref_entite_path,
                    ref_transfo_path=ref_transfo_path,
                    ref_qis_path=ref_qis_path,
                    ref_adf_qis_path=ref_adf_qis_path,
                    ref_dzone_qis_path=ref_dzone_qis_path,
                    run_timestamp=run_timestamp,
                    export_type=export_type,
                )

                result_after_entite = qis_processor.filter_and_join_ref_entite(data_import_filtered)
                result_after_transfo = qis_processor.join_with_ref_transfo(result_after_entite)
                result_with_dzone_qis = qis_processor.join_with_ref_dzone_qis(result_after_transfo)
                result_with_qis = qis_processor.join_with_ref_qis(result_with_dzone_qis)
                grouped_result = qis_processor.group_and_sum_unadjusted_p_amount(result_with_qis)
                pivoted_and_reordered_result = qis_processor.pivot_and_reorder(grouped_result)
                final_result_with_adf_qis = qis_processor.join_with_ref_adf_qis(pivoted_and_reordered_result)
                final_result = qis_processor.add_adjusted_amounts(final_result_with_adf_qis)

                # Transition vers le fichier template
                buffer = apply_to_template(final_result, input_excel_path)

                # Ajouter au ZIP
                folder_path_global = f"{base_folder}/{currency}/Reports_all_entities"
                file_name_global = f"{folder_path_global}/QIS_{export_type}_{currency}_All_Entities.xlsx"
                zipf.writestr(file_name_global, buffer.getvalue())

                # Ne générer que les rapports globaux si export_type == 'ALL'
                if export_type == 'ALL':
                    continue
                else:
                    # Sauvegarder les fichiers par entité
                    for entity in final_result["Ref_Entite.entité"].unique():
                        entity_data = final_result[final_result["Ref_Entite.entité"] == entity]
                        if entity_data.empty:
                            continue
                        buffer_entity = apply_to_template(entity_data, input_excel_path)
                        folder_path_entity = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
                        file_name_entity = f"{folder_path_entity}/QIS_{export_type}_{currency}_{entity}.xlsx"
                        zipf.writestr(file_name_entity, buffer_entity.getvalue())

    print("Tous les fichiers QIS ont été ajoutés au ZIP.")

def process_almm(preprocessed_data,
    data_path, ref_entite_path, ref_transfo_path, ref_almm_path, ref_adf_almm_path,
    ref_dzone_almm_path, input_excel_path, run_timestamp, export_type, zip_buffer, entity=None, currency=None, indicator="ALL"
):
    """
    Processus pour traiter les données ALMM avec gestion spécifique des exports dans un ZIP.
    """
    base_folder = f"RUN_{run_timestamp}_{export_type}"  # Dossier racine dans le ZIP

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        if export_type == "GRAN":
            if not entity or not currency:
                raise ValueError("Pour un export de type GRAN, une entité et une devise spécifiques doivent être fournies.")

            print(f"Traitement GRAN pour l'entité '{entity}' et la devise '{currency}'...")

            # Filtrer les données pour GRAN
            if isinstance(preprocessed_data, pd.DataFrame):
                # Si c'est un DataFrame, afficher ses colonnes
                if "D_CU" not in preprocessed_data.columns:
                    raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                if currency == "ALL":
                    filtered_data = preprocessed_data
                else:
                    filtered_data = preprocessed_data[preprocessed_data["D_CU"] == currency]
            elif isinstance(preprocessed_data, dict):
                # Si c'est un dictionnaire, accéder à la clé "filtered_data"
                if "filtered_data" in preprocessed_data:
                    filtered_data = preprocessed_data["filtered_data"]
                    if "D_CU" not in filtered_data.columns:
                        raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                    if currency == "ALL":
                        filtered_data = filtered_data
                    else:
                        filtered_data = filtered_data[filtered_data["D_CU"] == currency]
                else:
                    raise ValueError("La clé 'filtered_data' est absente dans preprocessed_lcr_data.")
            else:
                raise TypeError("preprocessed_lcr_data doit être un DataFrame ou un dictionnaire.")

            # Vérifier si 'filtered_data' est valide
            if filtered_data.empty:
                st.error(f"Aucune donnée trouvée pour la devise '{currency}' dans l'export GRAN.")


            # Étape 2 : Filtrer par indicateur
            if indicator == "BILAN":
                filtered_data = filtered_data[filtered_data["D_T1"] == "INTER"]
            elif indicator == "CONSO":
                filtered_data = filtered_data[filtered_data["D_T1"] != "INTER"]
            elif indicator == "ALL":
                filtered_data = filtered_data
            else:
                raise ValueError("Indicateur non pris en charge. Choisissez parmi ALL, BILAN, ou CONSO.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour l'indicateur '{indicator}'.")

            # Initialiser la classe ALMM
            almm_processor = ALMM(
                data_import=filtered_data,
                ref_entite_path=ref_entite_path,
                ref_transfo_path=ref_transfo_path,
                ref_almm_path=ref_almm_path,
                ref_adf_almm_path=ref_adf_almm_path,
                ref_dzone_almm_path=ref_dzone_almm_path,
                run_timestamp=run_timestamp,
                export_type=export_type,
            )

            # Appliquer les transformations
            result_after_entite = almm_processor.filter_and_join_ref_entite(filtered_data)
            result_after_transfo = almm_processor.join_with_ref_transfo(result_after_entite)
            result_with_dzone_almm = almm_processor.join_with_ref_dzone_almm(result_after_transfo)
            result_with_almm = almm_processor.join_with_ref_almm(result_with_dzone_almm)
            grouped_result = almm_processor.group_and_sum_unadjusted_p_amount(result_with_almm)
            pivoted_and_reordered_result = almm_processor.pivot_and_reorder(grouped_result)
            final_result_with_adf_almm = almm_processor.join_with_ref_adf_almm(pivoted_and_reordered_result)
            final_result = almm_processor.add_adjusted_amounts(final_result_with_adf_almm)

            # Filtrer par entité
            final_result = final_result[final_result["Ref_Entite.entité"] == entity]

            # Sauvegarder dans le ZIP
            folder_path = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
            file_name = f"{folder_path}/ALMM_GRAN_{currency}_{entity}.xlsx"
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_file_path = os.path.join(temp_dir, "temp_output.xlsx")
                try:
                    final_result.to_excel(temp_file_path, index=False, engine="xlsxwriter")
                    zipf.write(temp_file_path, arcname=file_name)
                except PermissionError as e:
                    print(f"Erreur de permission lors de la création du fichier : {e}")
                except Exception as e:
                    print(f"Une erreur inattendue s'est produite : {e}")

        else:  # Cas ALL, BILAN, CONSO
            for currency, file_path in preprocessed_data.items():
                if not os.path.exists(file_path):
                    print(f"Le fichier {file_path} n'existe pas. Aucun traitement pour cette devise.")
                    continue

                try:
                    data_import_filtered = pd.read_excel(file_path, engine="openpyxl")
                except Exception as e:
                    print(f"Erreur lors de la lecture du fichier {file_path}: {e}")
                    continue

                if data_import_filtered.empty:
                    continue

                print(f"Traitement de la devise : {currency}")

                # Initialiser la classe ALMM
                almm_processor = ALMM(
                    data_import=data_import_filtered,
                    ref_entite_path=ref_entite_path,
                    ref_transfo_path=ref_transfo_path,
                    ref_almm_path=ref_almm_path,
                    ref_adf_almm_path=ref_adf_almm_path,
                    ref_dzone_almm_path=ref_dzone_almm_path,
                    run_timestamp=run_timestamp,
                    export_type=export_type,
                )

                # Appliquer les transformations
                result_after_entite = almm_processor.filter_and_join_ref_entite(data_import_filtered)
                result_after_transfo = almm_processor.join_with_ref_transfo(result_after_entite)
                result_with_dzone_almm = almm_processor.join_with_ref_dzone_almm(result_after_transfo)
                result_with_almm = almm_processor.join_with_ref_almm(result_with_dzone_almm)
                grouped_result = almm_processor.group_and_sum_unadjusted_p_amount(result_with_almm)
                pivoted_and_reordered_result = almm_processor.pivot_and_reorder(grouped_result)
                final_result_with_adf_almm = almm_processor.join_with_ref_adf_almm(pivoted_and_reordered_result)
                final_result = almm_processor.add_adjusted_amounts(final_result_with_adf_almm)

                # Sauvegarder le fichier global
                folder_path_global = f"{base_folder}/{currency}/Reports_all_entities"
                file_name_global = f"{folder_path_global}/ALMM_{export_type}_{currency}_All_Entities.xlsx"
                with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as temp_file:
                    final_result.to_excel(temp_file.name, index=False, engine="xlsxwriter")
                    zipf.write(temp_file.name, arcname=file_name_global)
                
                # Ne générer que les rapports globaux si export_type == 'ALL'
                if export_type == 'ALL':
                    continue
                else:
                    # Sauvegarder les fichiers par entité
                    for entity in final_result["Ref_Entite.entité"].unique():
                        entity_data = final_result[final_result["Ref_Entite.entité"] == entity]
                        if entity_data.empty:
                            continue
                        folder_path_entity = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
                        file_name_entity = f"{folder_path_entity}/ALMM_{export_type}_{currency}_{entity}.xlsx"
                        with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as temp_file:
                            entity_data.to_excel(temp_file.name, index=False, engine="xlsxwriter")
                            zipf.write(temp_file.name, arcname=file_name_entity)

    print("Tous les fichiers ALMM ont été ajoutés au ZIP.")


def process_nsfr(preprocessed_data,
                 data_path, ref_entite_path, ref_transfo_path, ref_nsfr_path, ref_adf_nsfr_path, ref_dzone_nsfr_path,
                 input_excel_path, run_timestamp, export_type, zip_buffer, entity=None, currency=None, indicator="ALL"):
    """
    Processus de traitement des données NSFR avec intégration des résultats dans un fichier template
    et gestion des exports structurés dans un ZIP.
    """
    if zip_buffer is None:
        raise ValueError("Le buffer ZIP n'est pas initialisé.")

    base_folder = f"RUN_{run_timestamp}_{export_type}"  # Dossier racine dans le ZIP

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        if export_type == "GRAN":
            if not entity or not currency:
                raise ValueError("Pour un export de type GRAN, une entité et une devise spécifiques doivent être fournies.")

            print(f"Traitement GRAN pour l'entité '{entity}' et la devise '{currency}'...")

            # Filtrer les données pour GRAN
            if isinstance(preprocessed_data, pd.DataFrame):
                if "D_CU" not in preprocessed_data.columns:
                    raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                if currency == "ALL":
                    filtered_data = preprocessed_data
                else:
                    filtered_data = preprocessed_data[preprocessed_data["D_CU"] == currency]
            elif isinstance(preprocessed_data, dict):
                if "filtered_data" in preprocessed_data:
                    filtered_data = preprocessed_data["filtered_data"]
                    if "D_CU" not in filtered_data.columns:
                        raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                    if currency == "ALL":
                        filtered_data = filtered_data
                    else:
                        filtered_data = filtered_data[filtered_data["D_CU"] == currency]
                else:
                    raise ValueError("La clé 'filtered_data' est absente dans preprocessed_data.")
            else:
                raise TypeError("preprocessed_data doit être un DataFrame ou un dictionnaire.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour la devise '{currency}' dans l'export GRAN.")

            # Étape 2 : Filtrer par indicateur
            if indicator == "BILAN":
                filtered_data = filtered_data[filtered_data["D_T1"] == "INTER"]
            elif indicator == "CONSO":
                filtered_data = filtered_data[filtered_data["D_T1"] != "INTER"]
            elif indicator != "ALL":
                raise ValueError("Indicateur non pris en charge. Choisissez parmi ALL, BILAN, ou CONSO.")

            if filtered_data.empty:
                raise ValueError(f"Aucune donnée trouvée pour l'indicateur '{indicator}'.")

            # Initialiser le processeur NSFR
            nsfr_processor = NSFR(
                data_import=filtered_data,
                ref_entite_path=ref_entite_path,
                ref_transfo_path=ref_transfo_path,
                ref_nsfr_path=ref_nsfr_path,
                ref_adf_nsfr_path=ref_adf_nsfr_path,
                ref_dzone_nsfr_path=ref_dzone_nsfr_path,
                run_timestamp=run_timestamp,
                export_type=export_type,
            )

            # Étapes de transformation
            result_after_entite = nsfr_processor.filter_and_join_ref_entite(filtered_data)
            result_after_transfo = nsfr_processor.join_with_ref_transfo(result_after_entite)
            result_with_dzone_nsfr = nsfr_processor.join_with_ref_dzone_nsfr(result_after_transfo)
            result_with_nsfr = nsfr_processor.join_with_ref_nsfr(result_with_dzone_nsfr)
            grouped_result = nsfr_processor.group_and_sum_unadjusted_p_amount(result_with_nsfr)
            pivoted_and_reordered_result = nsfr_processor.pivot_and_reorder(grouped_result)
            final_result_with_adf_nsfr = nsfr_processor.join_with_ref_adf_nsfr(pivoted_and_reordered_result)
            final_result = nsfr_processor.add_adjusted_amounts(final_result_with_adf_nsfr)

            # Filtrer par entité
            final_result = final_result[final_result["Ref_Entite.entité"] == entity]

            # Transition vers le fichier template
            buffer = apply_to_template(final_result, input_excel_path)

            # Ajouter au ZIP
            folder_path = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
            file_name = f"{folder_path}/NSFR_GRAN_{currency}_{entity}.xlsx"
            zipf.writestr(file_name, buffer.getvalue())

        else:  # Cas ALL, BILAN, CONSO
            for currency, file_path in preprocessed_data.items():
                if not os.path.exists(file_path):
                    print(f"Le fichier {file_path} n'existe pas. Aucun traitement pour cette devise.")
                    continue

                try:
                    data_import_filtered = pd.read_excel(file_path, engine="openpyxl")
                except Exception as e:
                    print(f"Erreur lors de la lecture du fichier {file_path}: {e}")
                    continue

                if data_import_filtered.empty:
                    continue

                print(f"Traitement de la devise : {currency}")

                nsfr_processor = NSFR(
                    data_import=data_import_filtered,
                    ref_entite_path=ref_entite_path,
                    ref_transfo_path=ref_transfo_path,
                    ref_nsfr_path=ref_nsfr_path,
                    ref_adf_nsfr_path=ref_adf_nsfr_path,
                    ref_dzone_nsfr_path=ref_dzone_nsfr_path,
                    run_timestamp=run_timestamp,
                    export_type=export_type,
                )

                # Étapes de transformation
                result_after_entite = nsfr_processor.filter_and_join_ref_entite(data_import_filtered)
                result_after_transfo = nsfr_processor.join_with_ref_transfo(result_after_entite)
                result_with_dzone_nsfr = nsfr_processor.join_with_ref_dzone_nsfr(result_after_transfo)
                result_with_nsfr = nsfr_processor.join_with_ref_nsfr(result_with_dzone_nsfr)
                grouped_result = nsfr_processor.group_and_sum_unadjusted_p_amount(result_with_nsfr)
                pivoted_and_reordered_result = nsfr_processor.pivot_and_reorder(grouped_result)
                final_result_with_adf_nsfr = nsfr_processor.join_with_ref_adf_nsfr(pivoted_and_reordered_result)
                final_result = nsfr_processor.add_adjusted_amounts(final_result_with_adf_nsfr)

                # Transition vers le fichier template global
                buffer = apply_to_template(final_result, input_excel_path)

                # Ajouter au ZIP
                folder_path = f"{base_folder}/{currency}/Reports_all_entities"
                file_name = f"{folder_path}/NSFR_{export_type}_{currency}_All_Entities.xlsx"
                zipf.writestr(file_name, buffer.getvalue())
                
                # Ne générer que les rapports globaux si export_type == 'ALL'
                if export_type == 'ALL':
                    continue
                
                
                else:
                    # Sauvegarder les fichiers par entité
                    for entity in final_result["Ref_Entite.entité"].unique():
                        entity_data = final_result[final_result["Ref_Entite.entité"] == entity]
                        if entity_data.empty:
                            continue
                        buffer_entity = apply_to_template(entity_data, input_excel_path)
                        folder_path_entity = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
                        file_name_entity = f"{folder_path_entity}/NSFR_{export_type}_{currency}_{entity}.xlsx"
                        zipf.writestr(file_name_entity, buffer_entity.getvalue())

    print("Tous les fichiers NSFR ont été ajoutés au ZIP.")



def process_lcr(preprocessed_lcr_data,
                data_path, ref_entite_path, ref_transfo_path, ref_lcr_path, ref_adf_lcr_path,
                input_excel_path, run_timestamp, export_type, zip_buffer, entity=None, currency=None, indicator="ALL"):
    """
    Processus de traitement des données LCR avec transition directe des données dans un fichier template
    et stockage des fichiers générés dans un ZIP en mémoire.
    """
    base_folder = f"RUN_{run_timestamp}_{export_type}"  # Dossier racine dans le ZIP

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        if export_type == "GRAN":
            if not entity or not currency:
                raise ValueError("Pour un export de type GRAN, une entité et une devise spécifiques doivent être fournies.")

            print(f"Traitement GRAN pour l'entité '{entity}' et la devise '{currency}'...")

            # Filtrer les données pour GRAN
            if isinstance(preprocessed_lcr_data, pd.DataFrame):
                if "D_CU" not in preprocessed_lcr_data.columns:
                    raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                if currency == "ALL":
                    filtered_data = preprocessed_lcr_data
                else:
                    filtered_data = preprocessed_lcr_data[preprocessed_lcr_data["D_CU"] == currency]
            elif isinstance(preprocessed_lcr_data, dict):
                if "filtered_data" in preprocessed_lcr_data:
                    filtered_data = preprocessed_lcr_data["filtered_data"]
                    if "D_CU" not in filtered_data.columns:
                        raise KeyError("La colonne 'D_CU' est absente dans les données prétraitées pour GRAN.")
                    if currency == "ALL":
                        filtered_data = filtered_data
                    else:
                        filtered_data = filtered_data[filtered_data["D_CU"] == currency]
                else:
                    raise ValueError("La clé 'filtered_data' est absente dans preprocessed_lcr_data.")
            else:
                raise TypeError("preprocessed_lcr_data doit être un DataFrame ou un dictionnaire.")

            # Vérification des données filtrées
            if filtered_data.empty:
                print(f"Attention : aucune donnée trouvée pour la devise '{currency}' avec export GRAN.")
                return

            # Initialiser le processeur LCR
            lcr_processor = LCR(
                data_import=filtered_data,
                ref_entite_path=ref_entite_path,
                ref_transfo_path=ref_transfo_path,
                ref_lcr_path=ref_lcr_path,
                ref_adf_lcr_path=ref_adf_lcr_path,
                input_excel_path=input_excel_path,
                run_timestamp=run_timestamp,
                export_type=export_type,
            )

            # Étapes de transformation
            result_after_entite = lcr_processor.filter_and_join_ref_entite(filtered_data)
            result_after_transfo = lcr_processor.join_with_ref_transfo(result_after_entite)
            result_after_lcr = lcr_processor.join_with_ref_lcr(result_after_transfo)
            result_with_amount = lcr_processor.add_unadjusted_p_amount(result_after_lcr)
            grouped_result = lcr_processor.group_and_sum(result_with_amount)
            result_with_adf = lcr_processor.join_with_ref_adf_lcr(grouped_result)
            final_result = lcr_processor.add_adjusted_amount(result_with_adf)
            final_result = final_result[final_result["Ref_Entite.entité"] == entity]

            # Vérification des données finales
            if final_result.empty:
                print(f"Aucune donnée à exporter pour l'entité '{entity}' et la devise '{currency}'.")
                return

            # Transition vers le fichier template
            buffer = apply_to_template(final_result, input_excel_path)

            if buffer.getvalue() == b"":
                print("Le buffer est vide ! Vérifiez la fonction apply_to_template.")
                return

            # Ajouter au ZIP
            folder_path = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
            file_name = f"{folder_path}/LCR_GRAN_{currency}_{entity}.xlsx"
            print(f"Écriture dans le ZIP : {file_name}")
            zipf.writestr(file_name, buffer.getvalue())

        else:  # Pour ALL, BILAN, CONSO
            for currency, filtered_data in preprocessed_lcr_data.items():
                if isinstance(filtered_data, str):
                    try:
                        filtered_data = pd.read_excel(filtered_data, engine="openpyxl")
                    except Exception as e:
                        print(f"Erreur lors de la lecture du fichier {filtered_data}: {e}")
                        continue

                if filtered_data.empty:
                    continue

                print(f"Traitement de la devise : {currency}")

                # Initialiser le processeur LCR
                lcr_processor = LCR(
                    data_import=filtered_data,
                    ref_entite_path=ref_entite_path,
                    ref_transfo_path=ref_transfo_path,
                    ref_lcr_path=ref_lcr_path,
                    ref_adf_lcr_path=ref_adf_lcr_path,
                    input_excel_path=input_excel_path,
                    run_timestamp=run_timestamp,
                    export_type=export_type,
                )

                # Transformation des données
                result_after_entite = lcr_processor.filter_and_join_ref_entite(filtered_data)
                result_after_transfo = lcr_processor.join_with_ref_transfo(result_after_entite)
                result_after_lcr = lcr_processor.join_with_ref_lcr(result_after_transfo)
                result_with_amount = lcr_processor.add_unadjusted_p_amount(result_after_lcr)
                grouped_result = lcr_processor.group_and_sum(result_with_amount)
                result_with_adf = lcr_processor.join_with_ref_adf_lcr(grouped_result)
                final_result = lcr_processor.add_adjusted_amount(result_with_adf)


                # Transition vers le fichier template global
                buffer = apply_to_template(final_result, input_excel_path)

                # Ajouter au ZIP
                folder_path_global = f"{base_folder}/{currency}/Reports_all_entities"
                file_name_global = f"{folder_path_global}/LCR_{export_type}_{currency}_All_Entities.xlsx"
                with zipfile.ZipFile(zip_buffer, "a") as zipf:
                    zipf.writestr(file_name_global, buffer.getvalue())
                    
                # Ne générer que les rapports globaux si export_type == 'ALL'
                if export_type == 'ALL':
                    continue
                else:
                    # Sauvegarder les fichiers par entité
                    for entity in final_result["Ref_Entite.entité"].unique():
                        entity_data = final_result[final_result["Ref_Entite.entité"] == entity]
                        if entity_data.empty:
                            continue
                        buffer_entity = apply_to_template(entity_data, input_excel_path)
                        folder_path_entity = f"{base_folder}/{currency}/Reports_by_entity/{entity}"
                        file_name_entity = f"{folder_path_entity}/LCR_{export_type}_{currency}_{entity}.xlsx"
                        with zipfile.ZipFile(zip_buffer, "a") as zipf:
                            zipf.writestr(file_name_entity, buffer_entity.getvalue())


def apply_to_template(dataframe, template_path):
    """
    Applique les données d'un DataFrame dans un fichier de template.
    Les colonnes du DataFrame doivent correspondre exactement à celles du template.
    
    :param dataframe: DataFrame contenant les données à insérer.
    :param template_path: Chemin du fichier Excel template.
    :return: Un buffer contenant le fichier Excel modifié.
    """
    buffer = BytesIO()
    
    # Charger le template
    workbook = load_workbook(template_path)
    sheet = workbook.active  # Utiliser la première feuille

    # Effacer les données existantes dans la feuille (à partir de la 2e ligne)
    for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, min_col=1, max_col=sheet.max_column):
        for cell in row:
            cell.value = None

    # Insérer les données du DataFrame dans le template
    for i, row in enumerate(dataframe.values, start=2):  # Commence à la ligne 2
        for j, value in enumerate(row, start=1):  # Commence à la colonne 1
            sheet.cell(row=i, column=j, value=value)

    # Sauvegarder dans un buffer en mémoire
    workbook.save(buffer)
    buffer.seek(0)
    return buffer

def add_file_to_zip(zip_buffer, file_path, arcname):
    """
    Ajoute un fichier au buffer ZIP avec une gestion des erreurs.

    :param zip_buffer: Buffer ZIP en mémoire.
    :param file_path: Chemin absolu du fichier à ajouter.
    :param arcname: Nom du fichier à l'intérieur du ZIP.
    """
    try:
        with zipfile.ZipFile(zip_buffer, mode="a") as zipf:
            zipf.write(file_path, arcname=arcname)
    except Exception as e:
        raise RuntimeError(f"Erreur lors de l'ajout du fichier {file_path} au ZIP : {e}")
                            
def validate_zip_content(zip_buffer, expected_files):
    """
    Valide que tous les fichiers attendus sont dans le buffer ZIP.

    :param zip_buffer: Buffer ZIP en mémoire.
    :param expected_files: Liste des chemins attendus à l'intérieur du ZIP.
    """
    with zipfile.ZipFile(zip_buffer, 'r') as zipf:
        zip_contents = zipf.namelist()
        missing_files = [file for file in expected_files if file not in zip_contents]
        if missing_files:
            raise ValueError(f"Les fichiers suivants manquent dans le ZIP : {missing_files}")


def execute_processes_in_parallel(processes):
    """
    Exécute plusieurs processus en parallèle.

    :param processes: Liste de tuples contenant une fonction à exécuter et ses arguments.
    Format : [(fonction, (arg1, arg2, ...)), ...]
    :return: Résultats et erreurs des processus.
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    results = {}
    errors = {}

    with ThreadPoolExecutor() as executor:
        # Soumettre toutes les tâches
        future_to_process = {
            executor.submit(func, *args): func.__name__ for func, args in processes
        }

        for future in as_completed(future_to_process):
            func_name = future_to_process[future]
            try:
                result = future.result()  # Récupérer le résultat de la fonction
                results[func_name] = result
            except Exception as e:
                errors[func_name] = str(e)  # Capturer l'exception

    return results, errors


def save_hierarchy_to_excel_from_directory(base_dir, output_file):
    """
    Sauvegarde la hiérarchie des fichiers et dossiers dans un fichier Excel.
    """
    def extract_hierarchy_from_paths(base_dir):
        hierarchy = {}
        seen_files = set()
        for root, dirs, files in os.walk(base_dir):
            relative_path = os.path.relpath(root, base_dir)
            levels = relative_path.split(os.sep) if relative_path != "." else []
            current_level = hierarchy
            for level in levels:
                current_level = current_level.setdefault(level, {})
            for file in files:
                file_path = os.path.join(relative_path, file)
                if file_path not in seen_files:
                    current_level[file] = None
                    seen_files.add(file_path)
        return hierarchy
    
    hierarchy = extract_hierarchy_from_paths(base_dir)
    rows = []

    def traverse_hierarchy(parent, structure, level=0):
        if isinstance(structure, dict):
            for key, value in structure.items():
                rows.append((level, key))
                traverse_hierarchy(key, value, level + 1)
        elif structure is None:
            rows.append((level, parent))

    traverse_hierarchy(None, hierarchy)

    if not rows:
        print(f"Aucune hiérarchie trouvée dans le dossier : {base_dir}")
        return

    max_depth = max(level for level, _ in rows) + 1
    data = []
    for level, name in rows:
        row = [''] * (level + 1)
        row[level] = name
        data.append(row)

    df = pd.DataFrame(data, columns=[f"Level {i+1}" for i in range(max_depth)])

    # Sauvegarder dans un buffer en mémoire
    with io.BytesIO() as buffer:
        with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False)
        buffer.seek(0)
        with open(output_file, "wb") as f:
            f.write(buffer.read())
    print(f"Hiérarchie sauvegardée dans le fichier : {output_file}")

def replace_duplicates_with_nan(hierarchy_df):

    for column in hierarchy_df.columns:
        seen_values = []  # Liste pour suivre les valeurs uniques
        hierarchy_df[column] = hierarchy_df[column].apply(
            lambda x: x if x not in seen_values and not seen_values.append(x) else float('nan')
        )
    return hierarchy_df


def remove_duplicate_xlsx_files(hierarchy_df: pd.DataFrame) -> pd.DataFrame:
    """
    Supprime les doublons de fichiers Excel (.xlsx) dans un DataFrame hiérarchique.
    """
    seen_files = set()
    rows_to_keep = []

    for index, row in hierarchy_df.iterrows():
        file_name = row.iloc[-1]  # Supposons que le dernier niveau contient les noms de fichiers
        if isinstance(file_name, str) and file_name.endswith(".xlsx"):
            if file_name not in seen_files:
                seen_files.add(file_name)
                rows_to_keep.append(index)
        else:
            rows_to_keep.append(index)

    return hierarchy_df.loc[rows_to_keep].reset_index(drop=True)

def extract_hierarchy_from_zip(zip_buffer):
    """
    Extrait la hiérarchie des fichiers d'un ZIP en mémoire et structure la sortie en niveaux,
    avec suppression des doublons pour chaque niveau, sauf pour le Level 1.
    :param zip_buffer: Le buffer ZIP en mémoire.
    :return: Un DataFrame représentant la hiérarchie des fichiers dans le ZIP.
    """
    with zipfile.ZipFile(zip_buffer, 'r') as zipf:
        file_list = zipf.namelist()  # Liste des fichiers dans le ZIP

    # Construire la hiérarchie
    hierarchy = {}
    for file_path in file_list:
        parts = file_path.split('/')
        current_level = hierarchy
        for part in parts:
            if part not in current_level:
                current_level[part] = {}
            current_level = current_level[part]

    # Fonction récursive pour transformer la hiérarchie en lignes
    def traverse_hierarchy(node, depth=0, path=[]):
        rows = []
        for key, value in node.items():
            new_path = path + [''] * (depth - len(path)) + [key]
            rows.append(new_path)
            if isinstance(value, dict):  # Si c'est un dossier, continuer la traversée
                rows.extend(traverse_hierarchy(value, depth + 1, new_path))
            else:
                rows.append(new_path + [value])  # Ajouter les fichiers
        return rows

    # Extraire les lignes structurées
    rows = traverse_hierarchy(hierarchy)

    # Supprimer les doublons dans les colonnes en insérant des cellules vides pour éviter la répétition
    def remove_redundancy(rows):
        """
        Supprime les redondances dans les lignes du tableau en conservant uniquement
        les colonnes pertinentes.

        :param rows: Liste de listes représentant les lignes du tableau.
        :return: Liste de lignes nettoyées.
        """
        if not rows:
            raise ValueError("Le tableau 'rows' est vide. Vérifiez les données entrantes.")

        if not isinstance(rows[0], list):
            raise ValueError("Le tableau 'rows' doit être une liste de listes.")

        for col in range(1, len(rows[0])):  # Parcours des colonnes, sauf Level 1 (colonne 0)
            # Ajoutez votre logique pour traiter les colonnes ici.
            pass

        return rows

    if rows :
        rows = remove_redundancy(rows)

        # Trouver la profondeur maximale
        max_depth = max(len(row) for row in rows)

        # Compléter les lignes avec des colonnes vides jusqu'à la profondeur maximale
        structured_rows = [row + [''] * (max_depth - len(row)) for row in rows]

        # Construire un DataFrame
        
        df = pd.DataFrame(structured_rows, columns=[f"Level {i}" for i in range(0,max_depth)])
        return df

def process_generic(data, ref_paths, run_timestamp, export_type, zip_buffer, entity=None, currency=None):
    """
    Exemple générique d'une fonction de traitement écrivant dans le ZIP.
    """
    base_folder = f"RUN_{run_timestamp}_{export_type}"

    with zipfile.ZipFile(zip_buffer, 'a') as zipf:
        try:
            # Simulez une transformation et écrivez les résultats
            result_data = pd.DataFrame({"Col1": [1, 2], "Col2": [3, 4]})
            folder_path = f"{base_folder}/Example_Process"
            file_name = f"{folder_path}/Result.xlsx"

            # Créez un fichier temporaire et ajoutez-le au ZIP
            with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as temp_file:
                result_data.to_excel(temp_file.name, index=False, engine="xlsxwriter")
                zipf.write(temp_file.name, arcname=file_name)

            print(f"Fichier ajouté au ZIP : {file_name}")
        except Exception as e:
            raise ValueError(f"Erreur lors de l'ajout des résultats au ZIP : {e}")



    
def count_entity_occurrences_from_df(export_type: str, hierarchy_df: pd.DataFrame, 
                                     chosen_entities: list = None, chosen_indicator: str = "ALL") -> tuple:

    # Nettoyer les espaces inutiles des colonnes
    hierarchy_df.columns = hierarchy_df.columns.str.strip()

    if export_type == "GRAN":
        # Validation des entrées
        if not chosen_entities:
            raise ValueError("Pour le type d'export GRAN, vous devez fournir une liste d'entités choisies.")
        
        # Créer un DataFrame pour les entités choisies, chaque entité ayant une occurrence de 1
        result_df = pd.DataFrame({
            'Entités': chosen_entities,
            'Nombre occurrences': [1] * len(chosen_entities)
        })

        # Définir le nombre d'occurrences pour les indicateurs
        num_entities = len(chosen_entities)
        if chosen_indicator == "ALL":
            # Tous les indicateurs reçoivent le même nombre d'occurrences
            indicators_df = pd.DataFrame({
                'indicateur': ['LCR', 'AER', 'NSFR', 'QIS', 'ALMM'],
                'nombre occurrences': [num_entities] * 5
            })
        else:
            # Seul l'indicateur choisi reçoit le nombre d'occurrences
            indicators_df = pd.DataFrame({
                'indicateur': [chosen_indicator],
                'nombre occurrences': [num_entities]
            })
        
        return result_df, indicators_df

    # Si l'export_type n'est pas GRAN, appliquer la logique standard
    try:
        all_index = hierarchy_df[hierarchy_df['Level 1'] == f'ALL'].index[0]
        eur_index = hierarchy_df[hierarchy_df['Level 1'] == f'EUR'].index[0]
        filtered_df_1 = hierarchy_df.iloc[all_index:eur_index + 1]
    except IndexError:
        st.write(hierarchy_df)
        raise ValueError(f"Les valeurs 'ALL' et 'EUR' ne sont pas présentes dans 'Level 1'.")

    # Filtrage de 'Level 2' avec 'Reports_by_entity'
    try:
        reports_by_entity_index = filtered_df_1[filtered_df_1['Level 2'] == 'Reports_by_entity'].index[0]
        filtered_df_2 = filtered_df_1.iloc[reports_by_entity_index:]
    except IndexError:
        raise ValueError(f"Le niveau 'Reports_by_entity' n'est pas trouvé dans 'Level 2'.")

    # Liste des entités et des comptages
    entity_list = []
    count_list = []

    # Variables pour suivre l'entité et ses occurrences
    last_entity = None
    current_count = 0

    # Variables pour compter les mots-clés spécifiques
    lcr_count = 0
    aer_count = 0
    nsfr_count = 0
    qis_count = 0
    almm_count = 0

    # Parcourir les lignes filtrées
    for _, row in filtered_df_2.iterrows():
        entity = row['Level 3']
        file_name = row['Level 4']

        # Si une nouvelle entité est rencontrée (valeur non nulle dans Level 3)
        if pd.notna(entity):
            # Sauvegarder le comptage de l'entité précédente si elle existe
            if last_entity is not None:
                entity_list.append(last_entity)
                count_list.append(current_count)
            # Réinitialiser pour la nouvelle entité
            last_entity = entity
            current_count = 0  # Réinitialiser le compteur pour la nouvelle entité

        # Si un fichier est présent dans Level 4, il est associé à l'entité courante
        if pd.notna(file_name):
            current_count += 1

            # Compter les occurrences des mots-clés spécifiques
            lcr_count += file_name.count('LCR_')
            aer_count += file_name.count('AER_')
            nsfr_count += file_name.count('NSFR_')
            qis_count += file_name.count('QIS_')
            almm_count += file_name.count('ALMM_')

    # Ajouter la dernière entité et son comptage (si applicable)
    if last_entity is not None:
        entity_list.append(last_entity)
        count_list.append(current_count)

    # Créer un DataFrame pour les entités et leur nombre d'occurrences
    result_df = pd.DataFrame({
        'Entités': entity_list,
        'Nombre occurrences': count_list
    })

    # Regrouper les entités ayant le même nom et additionner leurs occurrences
    grouped_result_df = result_df.groupby("Entités", as_index=False).agg({"Nombre d'occurrences": "sum"})

    # Créer un DataFrame pour les mots-clés spécifiques
    indicators_df = pd.DataFrame({
        'indicateur': ['LCR', 'AER', 'NSFR', 'QIS', 'ALMM'],
        'nombre occurrences': [lcr_count, aer_count, nsfr_count, qis_count, almm_count]
    })

    return grouped_result_df, indicators_df

def save_to_excel(data: pd.DataFrame, template_path: str, output_path: str, zip_buffer: zipfile.ZipFile):
    """
    Sauvegarde les données dans un fichier Excel en utilisant un template et ajoute le fichier dans un ZIP.
    """
    workbook = load_workbook(template_path)
    first_sheet_name = workbook.sheetnames[0]
    first_sheet = workbook[first_sheet_name]

    # Nettoyage de la feuille existante
    for row in first_sheet.iter_rows():
        for cell in row:
            cell.value = None

    # Ajout des données
    for col_index, col_name in enumerate(data.columns, start=1):
        first_sheet.cell(row=1, column=col_index, value=col_name)  # En-têtes
        for row_index, value in enumerate(data[col_name], start=2):
            first_sheet.cell(row=row_index, column=col_index, value=value)

    # Sauvegarde dans un fichier temporaire
    temp_file = io.BytesIO()
    workbook.save(temp_file)
    temp_file.seek(0)

    # Ajout dans le ZIP
    zip_buffer.writestr(output_path, temp_file.getvalue())
    print(f"Fichier sauvegardé dans le ZIP : {output_path}")

def save_excel_with_structure(
    processed_data: dict,
    template_path: str,
    entity_list: list,
    run_timestamp: str,
    export_type: str,
    zip_buffer: zipfile.ZipFile,
    entity: str = None,
    currency: str = "ALL"
):

    base_folder = f"RUN_{run_timestamp}_{export_type}"

    if not processed_data:
        st.warning("Aucune donnée à sauvegarder dans le ZIP.")
        return

    for currency_key, data in processed_data.items():
        if not isinstance(data, pd.DataFrame) or data.empty:
            st.warning(f"Aucune donnée disponible pour la devise '{currency_key}'.")
            continue

        # Créer les chemins pour les fichiers globaux
        global_folder = f"{base_folder}/{currency_key}/Reports_all_entities"
        global_file = f"{global_folder}/LCR_{export_type}_{currency_key}_All_Entities.xlsx"

        # Sauvegarder uniquement le fichier global si export_type == 'ALL'
        if export_type == 'ALL':
            save_to_excel(data, template_path, global_file, zip_buffer)
            continue

        # Sinon, créer également les fichiers par entité
        entity_folder = f"{base_folder}/{currency_key}/Reports_by_entity"
        save_to_excel(data, template_path, global_file, zip_buffer)

        for specific_entity in entity_list:
            entity_data = data[data["Ref_Entite.entité"] == specific_entity]
            if not entity_data.empty:
                entity_file = f"{entity_folder}/{specific_entity}/LCR_{export_type}_{currency_key}_{specific_entity}.xlsx"
                save_to_excel(entity_data, template_path, entity_file, zip_buffer)

    st.success("Données sauvegardées avec succès dans le ZIP.")


def generate_import_files(uploaded_data, run_timestamp, zip_buffer, import_folder):
        """
        Génère les fichiers d'import BILAN et CONSO pour les devises ALL, EUR, et USD,
        et les ajoute dans un dossier compressé au sein du ZIP final.

        :param uploaded_data: DataFrame chargé depuis le fichier téléchargé.
        :param run_timestamp: Timestamp pour nommer le dossier d'import.
        :param zip_buffer: Buffer ZIP où les fichiers seront ajoutés.
        :param import_folder: Nom du dossier où placer les fichiers dans le ZIP.
        """
        # Filtrages
        bilan_data = uploaded_data[uploaded_data["D_T1"] == "INTER"]
        conso_data = uploaded_data[uploaded_data["D_T1"] != "INTER"]

        # Itération sur les devises
        for curr in ["ALL", "EUR", "USD"]:
            if curr == "ALL":
                bilan_filtered = bilan_data
                conso_filtered = conso_data
            else:
                bilan_filtered = bilan_data[bilan_data["D_CU"] == curr]
                conso_filtered = conso_data[conso_data["D_CU"] == curr]

            # Génération des fichiers
            bilan_file = f"{import_folder}/IMPORT_BILAN_{curr}.xlsx"
            conso_file = f"{import_folder}/IMPORT_CONSO_{curr}.xlsx"

            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as temp_bilan_file:
                bilan_filtered.to_excel(temp_bilan_file.name, index=False, engine="xlsxwriter")
                with zipfile.ZipFile(zip_buffer, "a") as zipf:
                    zipf.write(temp_bilan_file.name, arcname=bilan_file)

            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as temp_conso_file:
                conso_filtered.to_excel(temp_conso_file.name, index=False, engine="xlsxwriter")
                with zipfile.ZipFile(zip_buffer, "a") as zipf:
                    zipf.write(temp_conso_file.name, arcname=conso_file)

        # Sauvegarder le fichier importé brut
        imported_file = f"{import_folder}/IMPORT_SOURCE.xlsx"
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as temp_imported_file:
            uploaded_data.to_excel(temp_imported_file.name, index=False, engine="xlsxwriter")
            with zipfile.ZipFile(zip_buffer, "a") as zipf:
                zipf.write(temp_imported_file.name, arcname=imported_file)

        print(f"Fichiers d'import sauvegardés dans le dossier : {import_folder}")

if __name__ == "__main__":
    st.title("HIBISCUS Generator.")
    custom_css = """
        <style>
        /* Cacher le menu principal et le footer */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}

        /* Cacher le badge Viewer (Fork & GitHub) */
        .viewerBadge_container__1QSob {display: none !important;}
        .viewerBadge_link__yUdr6 {display: none !important;}

        /* Supprimer tout lien GitHub */
        a[href*="github.com"] {display: none !important;}
        </style>
    """
    st.markdown(custom_css, unsafe_allow_html=True)
    run_timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    # CSS pour forcer les boutons à occuper toute la largeur de la barre latéral
    # Initialiser l'état de navigation
    if "menu_choice" not in st.session_state:
        st.session_state.menu_choice = "Main"  # Page par défaut
    # CSS pour forcer les boutons à occuper toute la largeur de la barre latérale
    st.markdown(
        """
        <style>
        .sidebar-buttons {
            display: flex;
            flex-direction: column;
            gap: 10px;
            width : 100%;
        }
        .stButton button {
            all: unset;
            display: block;
            width: 100%;  /* Force le bouton à occuper toute la largeur */
            color: white;
            border: 1px solid grey;
            padding: 10px;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            text-align: center;
            box-sizing: border-box; /* Assure que le padding est inclus dans la largeur */
            display :flex;
            align-items : left;
            justify-content : left;
        }
        .sidebar-buttons button:hover {
            background-color: #105ea2;
        }
        .stButton button.active {
            background-color: #0d4d8c;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # Barre latérale avec des boutons stylisés
    st.sidebar.title("Menu")
    st.sidebar.markdown('<div class="sidebar-buttons">', unsafe_allow_html=True)

    if st.sidebar.button("Main", key="main_button"):
        st.session_state.menu_choice = "Main"
    if st.sidebar.button("Export", key="export_button"):
        st.session_state.menu_choice = "Export"
    if st.sidebar.button("Fonctionnalités", key="features_button"):
        st.session_state.menu_choice = "Fonctionnalités"

    st.sidebar.markdown("</div>", unsafe_allow_html=True)




    if st.session_state.menu_choice == "Main" :
        # Introduction structurée pour l'application
        st.markdown(
            """
            <style>
                .intro-header {
                    font-size: 32px;
                    font-weight: bold;
                    color: #2E86C1;
                    text-align: center;
                }
                .intro-subheader {
                    font-size: 18px;
                    color: #5D6D7E;
                    text-align: center;
                    margin-bottom: 20px;
                }
                .intro-description {
                    font-size: 16px;
                    color: white;
                    line-height: 1.6;
                }
                .highlight {
                    color: #D35400;
                    font-weight: bold;
                }
            </style>
            """, unsafe_allow_html=True
        )

        st.markdown('<div class="intro-header">🌟 HIBISCUS Generator 🌟</div>', unsafe_allow_html=True)
        st.markdown('<div class="intro-subheader">Un outil avancé pour générer des rapports financiers dynamiques</div>', unsafe_allow_html=True)

        st.markdown(
            """
            <div class="intro-description">
                Bienvenue dans <span class="highlight">HIBISCUS Generator</span>, une application web conçue pour simplifier et 
                automatiser le traitement des données financières hiérarchiques. Grâce à cet outil, vous pouvez :
                <ul>
                    <li>📊 Générer des rapports structurés pour différents types d'exports (<span class="highlight">ALL</span>, <span class="highlight">BILAN</span>, <span class="highlight">CONSO</span>, et <span class="highlight">GRAN</span>).</li>
                    <li>⚙️ Appliquer des processus financiers avancés comme <span class="highlight">NSFR</span>, <span class="highlight">LCR</span>, <span class="highlight">QIS</span>, <span class="highlight">ALMM</span>, et <span class="highlight">AER</span>.</li>
                    <li>📦 Exporter les résultats sous forme de fichiers compressés directement téléchargeables.</li>
                </ul>
                Laissez-vous guider par notre interface intuitive pour réaliser vos analyses financières en toute simplicité.
            </div>
            """,
            unsafe_allow_html=True
        )

    if st.session_state.menu_choice == "Export":
        
        st.subheader('Export des données:')
        
        st.markdown("""
                <style>
                .feature-header {
                    font-size: 24px;
                    font-weight: bold;
                    color: #2E86C1;
                    margin-top: 20px;
                    text-align: center;
                }
                .feature-description {
                    font-size: 16px;
                    color: #34495E;
                    line-height: 1.6;
                    margin-bottom: 20px;
                }
                .bold{
                    font-weight: bold;
                    color : lightgrey;
                }
                </style>
            """, unsafe_allow_html=True)
                
        # Téléchargement du fichier
        uploaded_file = st.sidebar.file_uploader("Téléchargez votre fichier Excel hiérarchique", type=["xlsx"])
        export_type = st.sidebar.selectbox("Choisissez le type d'export :", ["ALL", "BILAN", "CONSO", "GRAN"])
        run_timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        
        # Paramètres pour GRAN
        entity, currency, indicator, selected_processes = None, None, None, "ALL"
        if export_type == "GRAN":
            # Indicateur, Entité et Devise pour le GRAN
            indicator = st.sidebar.selectbox("Choisissez la vue :", ["ALL", "BILAN", "CONSO"])
            entity = st.sidebar.selectbox("Choisissez l'entité spécifique :", ["ALL"] + Entity_List)
            currency = st.sidebar.selectbox("Devise spécifique :", ["ALL","EUR", "USD"])
            selected_processes = st.sidebar.multiselect(
                "Sélectionnez les processus à exécuter :",
                ["ALL", "NSFR", "LCR", "QIS", "ALMM", "AER"],
                default="ALL"
            )

        # Lancer le traitement
        if st.sidebar.button("Lancer le traitement"):
            if uploaded_file:
                uploaded_data = pd.read_excel(uploaded_file)
                missing_columns = [col for col in expected_columns if col not in uploaded_data.columns]
                if missing_columns:
                    st.error("Certaines colonnes attendues sont manquantes dans le fichier :")

                    # Affichage des colonnes manquantes dans un tableau
                    missing_df = pd.DataFrame(
                        {"Colonnes manquantes": missing_columns}
                    )
                    st.markdown(
                        """
                        <style>
                            .missing-table {
                                border-radius: 5px;
                                padding: 10px;
                                margin-top: 10px;
                                margin-bottom: 10px;
                                box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
                            }
                            .missing-table h3 {
                                color: lightgrey;
                                margin-bottom: 10px;
                            }
                        </style>
                        """,
                        unsafe_allow_html=True,
                    )

                    # Convertir le tableau en HTML et l'afficher
                    st.markdown(
                        f"""
                        <div class="missing-table">
                            <h3>Colonnes manquantes :</h3>
                            {missing_df.to_html(index=False, escape=False, justify="center")}
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
                else:
                    try:
                        # Initialiser le buffer ZIP
                        zip_buffer = io.BytesIO()
                        
                        import_folder = f"import_{run_timestamp}"

                        with tempfile.TemporaryDirectory() as temp_dir:
                            # Sauvegarder le fichier téléchargé
                            input_file_path = os.path.join(temp_dir, "uploaded_hierarchy.xlsx")
                            with open(input_file_path, "wb") as f:
                                f.write(uploaded_file.getbuffer())

                            # Barre de progression et état actuel
                            progress_bar = st.progress(0)
                            current_task_placeholder = st.empty()

                            # Étape 1 : Prétraitement des données
                            current_task_placeholder.text("Prétraitement des données...")
                            generate_import_files(uploaded_data, run_timestamp, zip_buffer,import_folder)
                            preprocessed_data = preprocess_all_data(
                                data_path=input_file_path,
                                ref_entite_path="./Ref 2/ref_entite.xlsx",
                                ref_transfo_path="./Ref 2/ref_transfo_l1.xlsx",
                                ref_lcr_path="./Ref 2/ref_lcr.xlsx",
                                ref_adf_lcr_path="./Ref 2/ref_lcr_adf.xlsx",
                                input_excel_path="./Livrable/Templates/LCR_Template.xlsx",
                                run_timestamp=run_timestamp,
                                export_type=export_type,
                                currency=currency,
                            )
                            progress_bar.progress(20)
                            
                            # Vérification du type de données retournées
                            if export_type == "GRAN":
                                if "filtered_data" in preprocessed_data:
                                    gran_data = preprocessed_data["filtered_data"]
                                    generated_import_files = gran_data  # Le résultat contient les chemins des fichiers générés
                                    
                                else:
                                    st.write("Les données filtrées pour GRAN sont absentes.")
                            else:
                                # Pour les autres types d'export
                                generated_import_files = preprocessed_data  # Le résultat contient les chemins des fichiers générés
                                
                            progress_bar.progress(40)

                            # Étape 2 : Exécution des processus
                            current_task_placeholder.text("Exécution des processus...")
                            processes = {
                                "NSFR": {
                                    "func": process_nsfr,
                                    "args": [
                                        preprocessed_data, input_file_path, "./Ref 2/ref_entite.xlsx",
                                        "./Ref 2/ref_transfo_l1.xlsx", "./Ref 2/ref_nsfr.xlsx",
                                        "./Ref 2/ref_nsfr_adf.xlsx", "./Ref 2/ref_dzone_nsfr.xlsx",
                                        "./Livrable/Templates/NSFR_Template.xlsx", run_timestamp,
                                        export_type, zip_buffer, entity, currency, indicator
                                    ],
                                },
                                "LCR": {
                                    "func": process_lcr,
                                    "args": [
                                        preprocessed_data, input_file_path, "./Ref 2/ref_entite.xlsx",
                                        "./Ref 2/ref_transfo_l1.xlsx", "./Ref 2/ref_lcr.xlsx",
                                        "./Ref 2/ref_lcr_adf.xlsx", "./Livrable/Templates/LCR_Template.xlsx",
                                        run_timestamp, export_type, zip_buffer, entity, currency, indicator
                                    ],
                                },
                                "QIS": {
                                    "func": process_qis,
                                    "args": [
                                        preprocessed_data, input_file_path, "./Ref 2/ref_entite.xlsx",
                                        "./Ref 2/ref_transfo_l1.xlsx", "./Ref 2/Ref_QIS.xlsx",
                                        "./Ref 2/ref_nsfr_adf.xlsx", "./Ref 2/ref_dzone_nsfr.xlsx",
                                        "./Livrable/Templates/QIS_Template.xlsx", run_timestamp,
                                        export_type, zip_buffer, entity, currency, indicator
                                    ],
                                },
                                "ALMM": {
                                    "func": process_almm,
                                    "args": [
                                        preprocessed_data, input_file_path, "./Ref 2/ref_entite.xlsx",
                                        "./Ref 2/ref_transfo_l1.xlsx", "./Ref 2/ref_nsfr.xlsx",
                                        "./Ref 2/ref_nsfr_adf.xlsx", "./Ref 2/ref_dzone_nsfr.xlsx",
                                        "./Livrable/Templates/ALMM_Template.xlsx", run_timestamp,
                                        export_type, zip_buffer, entity, currency, indicator
                                    ],
                                },
                                "AER": {
                                    "func": process_aer,
                                    "args": [
                                        preprocessed_data, input_file_path, "./Ref 2/ref_entite.xlsx",
                                        "./Ref 2/ref_transfo_l1.xlsx", "./Ref 2/ref_aer.xlsx",
                                        "./Ref 2/ref_aer_adf.xlsx", "./Livrable/Templates/AER_Template.xlsx",
                                        run_timestamp, export_type, zip_buffer, entity, currency, indicator
                                    ],
                                },
                            }

                            if selected_processes == "ALL":
                                selected_processes = list(processes.keys())

                            step_progress = 40
                            for i, process_name in enumerate(selected_processes, start=1):
                                current_task_placeholder.text(f"Exécution du processus {process_name}...")
                                process_info = processes.get(process_name)
                                if process_info:
                                    process_info["func"](*process_info["args"])
                                else:
                                    print(f"Processus '{process_name}' non reconnu.")
                                progress_bar.progress(step_progress + (i * int(30 / len(selected_processes))))

                            current_task_placeholder.text("Génération des fichiers de hiérarchie...")
                            hierarchy_file_path = os.path.join(temp_dir, "hierarchy_all.xlsx")
                            hierarchy_df = extract_hierarchy_from_zip(zip_buffer)
                            hierarchy_df = replace_duplicates_with_nan(hierarchy_df)

                            hierarchy_df.to_excel(hierarchy_file_path, index=False)

                            current_task_placeholder.text("Ajout des fichiers au ZIP final...")
                            with zipfile.ZipFile(zip_buffer, "a") as zipf:

                                # Ajouter le fichier de hiérarchie
                                zipf.write(hierarchy_file_path, arcname="hierarchy_all.xlsx")
                                
                                # Ajouter le fichier des occurrences uniquement si ce n'est pas GRAN
                                if export_type == 'GRAN':
                                    if entity == "ALL":
                                        chosen_entities = Entity_List
                                    else:
                                        chosen_entities = [entity]

                                    if "ALL" in selected_processes:
                                        chosen_indicator = "ALL"
                                    else:
                                        # Concaténer les processus sélectionnés pour l'indicateur
                                        chosen_indicator = ", ".join(selected_processes)

                                    # Appel de la fonction pour GRAN
                                    grouped_count_df, indicators_df = count_entity_occurrences_from_df(
                                        export_type="GRAN",
                                        hierarchy_df=hierarchy_df,
                                        chosen_entities=chosen_entities,
                                        chosen_indicator=chosen_indicator
                                    )

                                    # Ajouter les entités manquantes avec 0 occurrences au DataFrame des entités
                                    all_entities = set(Entity_List)
                                    existing_entities = set(grouped_count_df["Entités"])
                                    missing_entities = all_entities - existing_entities

                                    # Ajouter les entités manquantes au DataFrame
                                    missing_df = pd.DataFrame({
                                        "Entités": list(missing_entities),
                                        "Nombre d'occurrences": [0] * len(missing_entities)
                                    })
                                    grouped_count_df = pd.concat([grouped_count_df, missing_df], ignore_index=True)

                                    # Générer le fichier Excel avec les résultats
                                    count_file_path = os.path.join(temp_dir, "count_gran.xlsx")
                                    with pd.ExcelWriter(count_file_path, engine='openpyxl') as writer:
                                        # Écrire le DataFrame des entités
                                        grouped_count_df.to_excel(writer, index=False, sheet_name="Résultats", startrow=0)

                                        # Ajouter 5 lignes vides avant le DataFrame des indicateurs
                                        start_row = len(grouped_count_df) + 6  # 1 ligne pour l'en-tête + 5 lignes vides
                                        indicators_df.to_excel(writer, index=False, sheet_name="Résultats", startrow=start_row)

                                    # Ajouter le fichier Excel dans le ZIP
                                    zipf.write(count_file_path, arcname="KPI_GRAN.xlsx")
                                
                                if export_type != "GRAN" and export_type != "ALL":
                                    count_file_path = os.path.join(temp_dir, "count_all.xlsx")
                                                                            
                                    grouped_count_df, indicators_df = count_entity_occurrences_from_df(export_type, hierarchy_df)
                                    
                                    # Ajouter les entités manquantes avec 0 occurrences au DataFrame des entités
                                    all_entities = set(Entity_List)
                                    existing_entities = set(grouped_count_df["Entités"])
                                    missing_entities = all_entities - existing_entities

                                    # Ajouter les entités manquantes au DataFrame
                                    missing_df = pd.DataFrame({
                                        "Entités": list(missing_entities),
                                        "Nombre d'occurrences": [0] * len(missing_entities)
                                    })
                                    grouped_count_df = pd.concat([grouped_count_df, missing_df], ignore_index=True)

                                    # Écrire les deux DataFrames dans un fichier Excel
                                    with pd.ExcelWriter(count_file_path, engine='openpyxl') as writer:
                                        # Écrire le premier DataFrame
                                        grouped_count_df.to_excel(writer, index=False, sheet_name="Résultats", startrow=0)
                                        
                                        # Ajouter 5 lignes vides avant le second DataFrame
                                        start_row = len(grouped_count_df) + 6  # 1 ligne pour l'en-tête + 5 lignes vides
                                        indicators_df.to_excel(writer, index=False, sheet_name="Résultats", startrow=start_row)
                                    
                                    # Ajouter le fichier Excel dans le ZIP
                                    zipf.write(count_file_path, arcname="KPI.xlsx")

                            progress_bar.progress(90)

                            # Proposer le téléchargement
                            zip_buffer.seek(0)
                            st.download_button(
                                label="Télécharger les résultats (ZIP)",
                                data=zip_buffer.getvalue(),
                                file_name=f"RUN_{run_timestamp}_{export_type}.zip",
                                mime="application/zip",
                            )
                            progress_bar.progress(100)
                            current_task_placeholder.success("Traitement terminé avec succès !")

                    except Exception as e:
                        import traceback
                        current_task_placeholder.text(f"Une erreur est survenue : {e}")
                        st.text("Traceback détaillé :")
                        st.text(traceback.format_exc())

        else:
            st.markdown('<div class="feature-description bold">Importez un fichier et choisissez la méthode pour exporter et autres filtres si nécessaire.</div>', unsafe_allow_html=True)


    elif st.session_state.menu_choice == "Fonctionnalités":
        st.subheader("Fonctionnalités de l'application")

        # Style global pour centrer le contenu des colonnes
        st.markdown(
            """
            <style>
            .custom-column2 {
                text-align: center;
                padding: 10px;
            }
            </style>
            """,
            unsafe_allow_html=True,
        )

        # Titre principal
        st.subheader("📄 Téléchargement de Documents")
        
        

        # Conteneur pour l'alignement des colonnes
        with st.container():
            # Colonnes
            doc_col1, doc_col2, doc_col3 = st.columns(3)

            # Bouton 1 : PowerPoint
            with doc_col1:
                st.markdown(
                    """
                    <div class="custom-column2">
                        <strong>Présentation du Projet (PowerPoint)</strong>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
                try:
                    with open("./Livrable/Tool_System/Hibiscus_livrable_version_client.pptx", "rb") as file:
                        st.download_button(
                            label="📤 Télécharger PowerPoint",
                            data=file,
                            file_name="presentation_hibiscus.pptx",
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            key="download_powerpoint",
                        )
                except FileNotFoundError:
                    st.error("Le fichier PowerPoint est introuvable.")
                except Exception as e:
                    st.error(f"Erreur lors du téléchargement : {e}")

            # Bouton 2 : PDF Logique
            with doc_col2:
                st.markdown(
                    """
                    <div class="custom-column2">
                        <strong>Présentation du guide utilisateur (PDF)</strong>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
                try:
                    with open("./Livrable/Tool_System/User Guide.pdf", "rb") as file:
                        st.download_button(
                            label="📤 Télécharger User Guide ",
                            data=file,
                            file_name="logic_documentation.pdf",
                            mime="application/pdf",
                            key="download_pdf_logic",
                        )
                except FileNotFoundError:
                    st.error("Le fichier PDF est introuvable.")
                except Exception as e:
                    st.error(f"Erreur lors du téléchargement : {e}")

            # Bouton 3 : Nouveau PDF
            with doc_col3:
                st.markdown(
                    """
                    <div class="custom-column2">
                        <strong>Documentation Technique (PDF)</strong>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
                try:
                    with open("./Livrable/Tool_System/Rapport_Specification_Fonctionnelle_Hibiscus.pdf", "rb") as file:
                        st.download_button(
                            label="📤 Télécharger PDF Technique",
                            data=file,
                            file_name="technical_documentation.pdf",
                            mime="application/pdf",
                            key="download_pdf_technique",
                        )
                except FileNotFoundError:
                    st.error("Le fichier PDF technique est introuvable.")
                except Exception as e:
                    st.error(f"Erreur lors du téléchargement : {e}")`,
                    'LCR.py' : `import os
import pandas as pd
from openpyxl import load_workbook
from datetime import datetime
import tempfile
import zipfile
import io

class LCR:
    def __init__(self, data_import: pd.DataFrame, ref_entite_path: str, ref_transfo_path: str, ref_lcr_path: str, ref_adf_lcr_path: str, input_excel_path: str, run_timestamp: str, export_type):


        self.data = data_import

        # Charger et prétraiter les fichiers de référence
        self.ref_entite = self.preprocess_ref_entite(ref_entite_path)
        self.ref_transfo = self.preprocess_ref_transfo(ref_transfo_path)
        self.ref_lcr = self.preprocess_ref_lcr(ref_lcr_path)
        self.ref_adf_lcr = self.preprocess_ref_adf_lcr(ref_adf_lcr_path)
        self.input_excel_path = input_excel_path
        self.run_timestamp = run_timestamp
        self.export_type = export_type

    
    
    def _save_import_files(self, filtered_data, import_folder, export_type):
        """
        Sauvegarde les fichiers d'import dans le dossier spécifié par devise (ALL, EUR, USD) 
        et vérifie que les fichiers générés ne sont pas corrompus.

        :param filtered_data: DataFrame filtré.
        :param import_folder: Dossier où sauvegarder les fichiers.
        :param export_type: Type d'export (ALL, BILAN, CONSO).
        :return: Dictionnaire contenant les chemins des fichiers générés.
        """
        saved_files = {}

        for currency in ["ALL", "EUR", "USD"]:
            if currency == "ALL":
                data_to_save = filtered_data
            else:
                data_to_save = filtered_data[filtered_data["D_CU"] == currency]

            # Vérifications avant sauvegarde
            if data_to_save.empty:
                print(f"Aucune donnée trouvée pour la devise {currency} dans {export_type}.")
                continue

            file_name = f"IMPORT_{export_type}_{currency}.xlsx"
            file_path = os.path.join(import_folder, file_name)

            try:
                # Étape 1 : Sauvegarder le fichier Excel
                data_to_save.to_excel(file_path, index=False, engine="xlsxwriter")
                print(f"Fichier généré : {file_path}")

                # Étape 2 : Valider que le fichier peut être relu correctement
                try:
                    test_read = pd.read_excel(file_path, engine="openpyxl")
                    if test_read.empty and not data_to_save.empty:
                        raise ValueError(f"Le fichier {file_path} est corrompu (lecture vide après écriture).")
                    if not data_to_save.equals(test_read):
                        raise ValueError(f"Le fichier {file_path} est corrompu (données lues non identiques à celles écrites).")
                except Exception as e:
                    raise ValueError(f"Validation échouée pour le fichier {file_path}: {e}")

                # Ajouter le fichier validé à la liste des fichiers sauvegardés
                saved_files[currency] = file_path

            except Exception as e:
                print(f"Erreur lors de la sauvegarde ou de la validation du fichier {file_path}: {e}")
                # Nettoyer le fichier corrompu s'il existe
                if os.path.exists(file_path):
                    os.remove(file_path)
                print(f"Fichier corrompu supprimé : {file_path}")

        return saved_files

    
    def preprocess_data(self, export_type="ALL", currency="ALL", entity="ALL"):
        """
        Nettoie et convertit les types des colonnes dans les données, génère les fichiers d'import
        pour BILAN, CONSO, ALL, et gère les étapes spécifiques pour GRAN.

        :param export_type: Type d'export choisi par l'utilisateur (ALL, BILAN, CONSO, GRAN).
        :param currency: Devise à filtrer (ALL, EUR, USD).
        :param entity: Entité à filtrer ou ALL.
        :return: Chemins des fichiers sauvegardés (dictionnaire) ou données filtrées (DataFrame) pour GRAN.
        """
        # Création du dossier d'import
        import_folder = f"./imports/import_{self.run_timestamp}"
        os.makedirs(import_folder, exist_ok=True)

        # Suppression des lignes totalement vides
        self.data = self.data.dropna(how="all")
        self.data = self.data[~self.data.apply(lambda row: all(row == ""), axis=1)]

        # Définition des types de colonnes
        column_types = {
            "D_CA": "string",
            "D_DP": "float64",
            "D_ZTFTR": "object",
            "D_PE": "float64",
            "D_RU": "string",
            "D_ORU": "string",
            "D_AC": "string",
            "D_FL": "string",
            "D_AU": "string",
            "D_T1": "object",
            "D_T2": "object",
            "D_CU": "string",
            "D_TO": "string",
            "D_GO": "string",
            "D_LE": "object",
            "D_NU": "object",
            "D_DEST": "object",
            "D_ZONE": "string",
            "D_MONNAIE": "string",
            "D_ENTITE": "object",
            "D_RESTIT": "object",
            "D_TYPCLI": "object",
            "D_SURFI": "object",
            "D_MU": "object",
            "D_PMU": "object",
            "D_ACTIVITE": "object",
            "D_ANALYSIS": "object",
            "D_PDT": "object",
            "P_AMOUNT": "Int64",
            "P_COMMENT": "object",
        }

        for col, dtype in column_types.items():
            if col in self.data.columns:
                try:
                    if dtype == "Int64":
                        self.data[col] = pd.to_numeric(self.data[col], errors='coerce').astype("Int64")
                    else:
                        self.data[col] = self.data[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Étape 1 : Filtrage spécifique pour GRAN
        if export_type == "GRAN":
            if currency == "ALL":
                raise ValueError("Pour un export de type GRAN, une devise spécifique doit être fournie.")

            print(f"Filtrage des données pour la devise '{currency}'...")
            filtered_data_currency = self.data[self.data["D_CU"] == currency]

            if filtered_data_currency.empty:
                raise ValueError(f"Aucune donnée trouvée pour la devise '{currency}'.")

            return filtered_data_currency

        generated_files = {}
        #Étape 2 : Génération des fichiers pour BILAN, CONSO, et ALL
        generated_files = {}
        if export_type in ["ALL", "BILAN","CONSO","GRAN"]:
            filtered_bilan = self.data[self.data["D_T1"] == "INTER"]
            filtered_conso = self.data[self.data["D_T1"] != "INTER"]
            generated_files.update(self._save_import_files(filtered_bilan, "BILAN", import_folder, filtered_conso, "CONSO"))
        print(f"Fichiers d'import sauvegardés dans : {import_folder}")
        return generated_files


    def _save_import_files(self, filtered_data_1, export_type_1, import_folder,filtered_data_2, export_type_2):
        """
        Sauvegarde les fichiers d'import dans le dossier spécifié par devise (ALL, EUR, USD).

        :param filtered_data: DataFrame filtré.
        :param import_folder: Dossier où sauvegarder les fichiers.
        :param export_type: Type d'export (ALL, BILAN, CONSO).
        :param all_data_accumulated: Liste pour accumuler les données.
        :return: Dictionnaire contenant les chemins des fichiers générés.
        """
        saved_files = {}

        for currency in ["ALL", "EUR", "USD"]:
            if currency == "ALL":
                data_to_save_1 = filtered_data_1
                data_to_save_2 = filtered_data_2
            else:
                data_to_save_1 = filtered_data_1[filtered_data_1["D_CU"] == currency]
                data_to_save_2 = filtered_data_2[filtered_data_2["D_CU"] == currency]

            if data_to_save_1.empty or data_to_save_2.empty:
                print(f"Aucune donnée trouvée pour la devise {currency} dans {export_type}.")
                continue

            file_name_1 = f"IMPORT_{export_type_1}_{currency}.xlsx"
            file_name_2 = f"IMPORT_{export_type_2}_{currency}.xlsx"
            file_path_1 = os.path.join(import_folder, file_name_1)
            file_path_2 = os.path.join(import_folder, file_name_2)
            try:
                data_to_save_1.to_excel(file_path_1, index=False, engine="xlsxwriter")
                data_to_save_2.to_excel(file_path_2, index=False, engine="xlsxwriter")
                print(f"Fichier généré : {file_path_1} et {file_path_2}")
                saved_files[currency] = file_path_1
                saved_files[currency] = file_path_2
            except Exception as e:
                print(f"Erreur lors de la génération du fichier {file_path_1} ou {file_path_2}: {e}")

        return saved_files




    def save_filtered_data(self, data: pd.DataFrame, file_name: str):
        """
        Sauvegarde les données filtrées dans un fichier Excel.

        :param data: DataFrame filtré à sauvegarder.
        :param file_name: Nom du fichier Excel de sortie.
        """
        file_path = f"./output/Exports/LCR/{file_name}" 
        os.makedirs(os.path.dirname(file_path), exist_ok=True) 
        data.to_excel(file_path, index=False, engine="openpyxl")
        print(f"Fichier sauvegardé : {file_path}")


    @staticmethod
    def preprocess_ref_entite(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_Entite.xlsx :
        - Supprime les lignes ayant une valeur nulle dans la colonne 'd_ru'.
        - Ajoute le préfixe 'Ref_Entite.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['d_ru'])  # Supprime les lignes où 'd_ru' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Entite.'
        df = df.rename(columns=lambda col: f"Ref_Entite.{col}")
        return df

    @staticmethod
    def preprocess_ref_transfo(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        df['Transfo_aggregate_L1'] = df['Transfo_aggregate_L1'].astype(str)  # Convertit en texte
        df = df.drop_duplicates(subset=['Transfo_aggregate_L1'])  # Supprime les doublons

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Transfo_L1.'
        df = df.rename(columns=lambda col: f"Ref_Transfo_L1.{col}")
        return df

    @staticmethod
    def preprocess_ref_lcr(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_LCR.xlsx :
        - Supprime les lignes où la valeur de 'Ligne_LCR' est nulle.
        - Ajoute le préfixe 'Ref_LCR.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['Ligne_LCR'])  # Supprime les lignes où 'Ligne_LCR' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_LCR.'
        df = df.rename(columns=lambda col: f"Ref_LCR.{col}")
        return df

    @staticmethod
    def preprocess_ref_adf_lcr(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_ADF_LCR.xlsx :
        - Change les types des colonnes selon les spécifications.
        - Ajoute le préfixe 'Ref_ADF_LCR.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        column_types = {
            "D_ru": "string",
            "Entité": "string",
            "D_ac": "string",
            "Indicator_Ligne": "string",
            "Indicator_ADF": "Int64",
        }

        # Appliquer les conversions de types
        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    if dtype == "Int64":
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes en ajoutant le préfixe 'Ref_ADF_LCR.'
        df = df.rename(columns=lambda col: f"Ref_ADF_LCR.{col}")
        return df


    def filter_and_join_ref_entite(self,preprocessed_data):

        # 2.2. Filtrer les données
        filtered_data = preprocessed_data[
            (preprocessed_data["D_FL"] != "T99") & (preprocessed_data["D_ZONE"].notna())
        ]
        
        # 2.3. Joindre la table principale filtrée avec Ref_Entite
        joined_data = pd.merge(
            filtered_data,  # Table principale filtrée
            self.ref_entite,  # Table secondaire Ref_Entite
            left_on="D_RU",  # Colonne de jointure dans la table principale
            right_on="Ref_Entite.d_ru",  # Colonne de jointure dans la table secondaire
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data
    
    def join_with_ref_transfo(self, filtered_data: pd.DataFrame):
        
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale (déjà filtrée et jointe avec Ref_Entite)
            self.ref_transfo,  # Référence Ref_Transfo_L1 (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_Transfo_L1.Transfo_aggregate_L1",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Transfo_aggregate_L1 n'est pas null
        filtered_joined_data = joined_data[joined_data["Ref_Transfo_L1.Transfo_aggregate_L1"].notna()]

        # Retourner les données après jointure et filtrage
        return filtered_joined_data

    def join_with_ref_lcr(self, filtered_data: pd.DataFrame):
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_lcr,  # Référence Ref_LCR (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_LCR.Compte Transfo",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data
    
    def add_unadjusted_p_amount(self, data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        required_columns = ["D_ZONE", "Ref_LCR.LCR_Flow_PCT", "Ref_LCR.LCR_Stock_PCT", "P_AMOUNT"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter la colonne 'Unadjusted_P_Amount'
        data["Unadjusted_P_Amount"] = data.apply(
            lambda row: row["Ref_LCR.LCR_Flow_PCT"] * row["P_AMOUNT"]
            if row["D_ZONE"] == "E01"
            else row["Ref_LCR.LCR_Stock_PCT"] * row["P_AMOUNT"],
            axis=1
        )

        return data

    
    def group_and_sum(self, data: pd.DataFrame):

        # Colonnes utilisées pour le regroupement
        group_columns = ["Ref_Entite.entité", "D_AC", "Ref_LCR.Ligne_LCR"]

        # Vérifier si les colonnes nécessaires sont présentes
        for col in group_columns + ["Unadjusted_P_Amount"]:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Regrouper les données et calculer la somme
        grouped_data = (
            data.groupby(group_columns, as_index=False)
            .agg(Sum_Unadjusted_P_Amount=("Unadjusted_P_Amount", "sum"))
        )

        # Retourner le DataFrame regroupé
        return grouped_data

    def join_with_ref_adf_lcr(self, grouped_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        required_columns_main = ["D_AC", "Ref_LCR.Ligne_LCR"]
        required_columns_ref = ["Ref_ADF_LCR.D_ac", "Ref_ADF_LCR.Indicator_Ligne"]
        
        for col in required_columns_main:
            if col not in grouped_data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame principal.")

        for col in required_columns_ref:
            if col not in self.ref_adf_lcr.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans Ref_ADF_LCR.")

        joined_data = pd.merge(
            grouped_data,  # Table principale après regroupement
            self.ref_adf_lcr,  # Référence Ref_ADF_LCR
            left_on=["D_AC", "Ref_LCR.Ligne_LCR"],  # Colonnes de jointure dans la table principale
            right_on=["Ref_ADF_LCR.D_ac", "Ref_ADF_LCR.Indicator_Ligne"],  # Colonnes de jointure dans la référence
            how="left", 
        )

        return joined_data

    def add_adjusted_amount(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # Vérifier que les colonnes nécessaires sont présentes
        required_columns = ["Sum_Unadjusted_P_Amount", "Ref_ADF_LCR.Indicator_ADF"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter la colonne 'P_Adjusted_Amount'
        data["P_Adjusted_Amount"] = data["Sum_Unadjusted_P_Amount"] * data["Ref_ADF_LCR.Indicator_ADF"]

        return data
    

    def save_to_excel(self, data: pd.DataFrame, template_path: str, output_path: str, zip_buffer: zipfile.ZipFile):
        """
        Sauvegarde les données dans un fichier Excel en utilisant un fichier template, directement dans un ZIP.
        """
        # Charger le classeur Excel existant
        workbook = load_workbook(template_path)
        first_sheet_name = workbook.sheetnames[0]
        first_sheet = workbook[first_sheet_name]

        # Effacer les anciennes données
        for row in first_sheet.iter_rows():
            for cell in row:
                cell.value = None

        # Insérer les nouvelles données
        for i, col_name in enumerate(data.columns, start=1):
            first_sheet.cell(row=1, column=i, value=col_name)  # Ajouter les en-têtes
            for j, value in enumerate(data[col_name], start=2):
                first_sheet.cell(row=j, column=i, value=value)

        # Sauvegarder dans un fichier temporaire
        temp_file = io.BytesIO()
        workbook.save(temp_file)
        temp_file.seek(0)

        # Ajouter dans le ZIP
        zip_buffer.writestr(output_path, temp_file.getvalue())
        print(f"Fichier sauvegardé dans le ZIP : {output_path}")

    def save_excel_with_structure(
        self,
        processed_data: dict,
        template_path: str,
        entity_list: list,
        run_timestamp: str,
        export_type: str,
        zip_buffer: zipfile.ZipFile,
        entity: str = None,
        currency: str = "ALL"
    ):
        base_folder = f"RUN_{run_timestamp}_{export_type}"

        if not processed_data:
            st.warning("Aucune donnée à sauvegarder dans le ZIP.")
            return

        if export_type in ["BILAN", "CONSO"]:
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame) or data.empty:
                    st.warning(f"Aucune donnée disponible pour la devise '{currency}'.")
                    continue

                global_file = f"{base_folder}/{export_type}_{currency}/Reports_all_entities/LCR_{export_type}_{currency}_All_Entities.xlsx"
                self.save_to_excel(data, template_path, global_file, zip_buffer)

                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    if not entity_data.empty:
                        entity_file = f"{base_folder}/{export_type}_{currency}/Reports_by_entity/{entity}/LCR_{export_type}_{currency}_{entity}.xlsx"
                        self.save_to_excel(entity_data, template_path, entity_file, zip_buffer)

        elif export_type == "ALL":
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame) or data.empty:
                    st.warning(f"Aucune donnée disponible pour la devise '{currency}'.")
                    continue

                global_file = f"{base_folder}/Reports_all_entities/LCR_ALL_All_Entities_{currency}.xlsx"
                self.save_to_excel(data, template_path, global_file, zip_buffer)

                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    if not entity_data.empty:
                        entity_file = f"{base_folder}/Reports_by_entity/{entity}/LCR_ALL_{currency}_{entity}.xlsx"
                        self.save_to_excel(entity_data, template_path, entity_file, zip_buffer)`,
                'NSFR.py' : `
import os
import pandas as pd
from openpyxl import load_workbook
from datetime import datetime  

class NSFR :
    def __init__(self, data_import: pd.DataFrame, ref_entite_path: str, ref_transfo_path: str, ref_nsfr_path: str, ref_adf_nsfr_path: str, ref_dzone_nsfr_path:str, run_timestamp: str, export_type : str):

        self.data = data_import

        # Charger et prétraiter les fichiers de référence
        self.ref_entite = self.preprocess_ref_entite(ref_entite_path)
        self.ref_transfo = self.preprocess_ref_transfo(ref_transfo_path)
        self.ref_nsfr = self.preprocess_ref_nsfr(ref_nsfr_path)
        self.ref_adf_nsfr = self.preprocess_ref_adf_nsfr(ref_adf_nsfr_path)
        self.ref_dzone_nsfr = self.preprocess_ref_dzone_nsfr(ref_dzone_nsfr_path)
        self.run_timestamp = run_timestamp
        export_type = export_type
        
        
    @staticmethod
    def preprocess_ref_entite(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_Entite.xlsx :
        - Supprime les lignes ayant une valeur nulle dans la colonne 'd_ru'.
        - Ajoute le préfixe 'Ref_Entite.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['d_ru'])  # Supprime les lignes où 'd_ru' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Entite.'
        df = df.rename(columns=lambda col: f"Ref_Entite.{col}")
        return df

    @staticmethod
    def preprocess_ref_transfo(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        df['Transfo_aggregate_L1'] = df['Transfo_aggregate_L1'].astype(str)  # Convertit en texte
        df = df.drop_duplicates(subset=['Transfo_aggregate_L1'])  # Supprime les doublons

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Transfo_L1.'
        df = df.rename(columns=lambda col: f"Ref_Transfo_L1.{col}")
        return df
    
    @staticmethod
    def preprocess_ref_nsfr(file_path: str) -> pd.DataFrame:

        df = pd.read_excel(file_path)

        # Changer les types des colonnes
        column_types = {
            "Compte Transfo": "string",
            "Ligne_NSFR": "string",
            "PCT_NSFR": "string",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe 'Ref_NSFR.'
        df = df.rename(columns=lambda col: f"Ref_NSFR.{col}")

        return df

    @staticmethod
    def preprocess_ref_adf_nsfr(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)

        column_types = {
            "D_ru": "string",
            "Entité": "string",
            "D_ac": "string",
            "Indicator_Ligne": "string",
            "Indicator_ADF": "Int64",
            "Indicator_ADF_0-6M": "float64",
            "Indicator_ADF_6-12M": "float64",
            "Indicator_ADF_>1Y": "float64",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    if dtype == "float64":
                        # Remplacer les valeurs non numériques
                        df[col] = pd.to_numeric(df[col].replace("NOT APPLICABLE", None), errors='coerce')
                    elif dtype == "Int64":
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe Ref_ADF_NSFR.
        df = df.rename(columns=lambda x: f"Ref_ADF_NSFR.{x}")

        return df

    @staticmethod
    def preprocess_ref_dzone_nsfr(file_path: str) -> pd.DataFrame:

        # Charger le fichier
        df = pd.read_excel(file_path)

        # Changer les types des colonnes
        column_types = {
            "Colonne1": "string",
            "Colonne2": "string",
        }
        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes
        rename_columns = {
            "Colonne1": "D_ZONE",
            "Colonne2": "NSFR_Bucket",
        }
        df = df.rename(columns=rename_columns)

        # Ignorer la première ligne
        df = df.iloc[1:].reset_index(drop=True)

        # Ajouter le préfixe 'Ref_DZONE_NSFR.' aux colonnes
        df = df.rename(columns=lambda col: f"Ref_DZONE_NSFR.{col}")

        return df

    def filter_and_join_ref_entite(self,preprocessed_data):

        # 2.2. Filtrer les données
        filtered_data = preprocessed_data[
            (preprocessed_data["D_FL"] != "T99") & (preprocessed_data["D_ZONE"].notna())
        ]
        
        # 2.3. Joindre la table principale filtrée avec Ref_Entite
        joined_data = pd.merge(
            filtered_data,  # Table principale filtrée
            self.ref_entite,  # Table secondaire Ref_Entite
            left_on="D_RU",  # Colonne de jointure dans la table principale
            right_on="Ref_Entite.d_ru",  # Colonne de jointure dans la table secondaire
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data

    def join_with_ref_transfo(self, filtered_data: pd.DataFrame):
    
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale (déjà filtrée et jointe avec Ref_Entite)
            self.ref_transfo,  # Référence Ref_Transfo_L1 (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_Transfo_L1.Transfo_aggregate_L1",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Transfo_aggregate_L1 n'est pas null
        filtered_joined_data = joined_data[joined_data["Ref_Transfo_L1.Transfo_aggregate_L1"].notna()]

        # Retourner les données après jointure et filtrage
        return filtered_joined_data

    
    def join_with_ref_dzone_nsfr(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_ZONE" not in filtered_data.columns:
            raise ValueError("La colonne 'D_ZONE' est manquante dans le DataFrame principal.")
        if "Ref_DZONE_NSFR.D_ZONE" not in self.ref_dzone_nsfr.columns:
            raise ValueError("La colonne 'Ref_DZONE_NSFR.D_ZONE' est manquante dans la table Ref_DZONE_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_dzone_nsfr,  # Référence Ref_DZONE_NSFR
            left_on="D_ZONE",  # Colonne de la table principale
            right_on="Ref_DZONE_NSFR.D_ZONE",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    def join_with_ref_nsfr(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in filtered_data.columns:
            raise ValueError("La colonne 'D_AC' est manquante dans le DataFrame principal.")
        if "Ref_NSFR.Compte Transfo" not in self.ref_nsfr.columns:
            raise ValueError("La colonne 'Ref_NSFR.Compte Transfo' est manquante dans la table Ref_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_nsfr,  # Référence Ref_NSFR
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_NSFR.Compte Transfo",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Ref_NSFR.Ligne_NSFR n'est pas null
        if "Ref_NSFR.Ligne_NSFR" not in joined_data.columns:
            raise ValueError("La colonne 'Ref_NSFR.Ligne_NSFR' est manquante dans le DataFrame après jointure.")
        filtered_joined_data = joined_data[joined_data["Ref_NSFR.Ligne_NSFR"].notna()]

        return filtered_joined_data
    
    def group_and_sum_unadjusted_p_amount(self, data: pd.DataFrame) -> pd.DataFrame:
    
        # Colonnes utilisées pour le regroupement
        group_columns = [
            "Ref_Entite.entité", 
            "D_AC", 
            "Ref_DZONE_NSFR.NSFR_Bucket", 
            "Ref_NSFR.Ligne_NSFR"
        ]

        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = group_columns + ["P_AMOUNT"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Regrouper les données et calculer la somme
        grouped_data = (
            data.groupby(group_columns, as_index=False)
            .agg(Unadjusted_P_Amount=("P_AMOUNT", "sum"))
        )

        return grouped_data

    def pivot_and_reorder(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_NSFR.Ligne_NSFR",
            "Ref_DZONE_NSFR.NSFR_Bucket",
            "Unadjusted_P_Amount",
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Pivoter les données
        pivoted_data = data.pivot_table(
            index=["Ref_Entite.entité", "D_AC", "Ref_NSFR.Ligne_NSFR"],  # Colonnes fixes
            columns="Ref_DZONE_NSFR.NSFR_Bucket",  # Colonne à pivoter
            values="Unadjusted_P_Amount",  # Valeur à agréger
            aggfunc="sum",  # Fonction d'agrégation
            fill_value=0,  # Remplir les valeurs manquantes par 0
        ).reset_index()

        # Réorganiser les colonnes
        desired_order = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_NSFR.Ligne_NSFR",
            "0-6M",
            "6-12M",
            ">1Y",
        ]
        for col in desired_order:
            if col not in pivoted_data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame pivoté.")

        reordered_data = pivoted_data[desired_order]

        return reordered_data

    
    def join_with_ref_adf_nsfr(self, data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in data.columns or "Ref_NSFR.Ligne_NSFR" not in data.columns:
            raise ValueError("Les colonnes 'D_AC' ou 'Ref_NSFR.Ligne_NSFR' sont manquantes dans la table principale.")
        if "Ref_ADF_NSFR.D_ac" not in self.ref_adf_nsfr.columns or "Ref_ADF_NSFR.Indicator_Ligne" not in self.ref_adf_nsfr.columns:
            raise ValueError("Les colonnes 'Ref_ADF_NSFR.D_ac' ou 'Ref_ADF_NSFR.Indicator_Ligne' sont manquantes dans la table Ref_ADF_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            data,  # Table principale
            self.ref_adf_nsfr,  # Référence Ref_ADF_NSFR
            left_on=["D_AC", "Ref_NSFR.Ligne_NSFR"],  # Colonnes de la table principale
            right_on=["Ref_ADF_NSFR.D_ac", "Ref_ADF_NSFR.Indicator_Ligne"],  # Colonnes de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    def add_adjusted_amounts(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "0-6M", 
            "6-12M", 
            ">1Y", 
            "Ref_ADF_NSFR.Indicator_ADF_0-6M", 
            "Ref_ADF_NSFR.Indicator_ADF_6-12M", 
            "Ref_ADF_NSFR.Indicator_ADF_>1Y"
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter les colonnes calculées
        data["P_Adjusted_Amount_0-6M"] = data["0-6M"] * data["Ref_ADF_NSFR.Indicator_ADF_0-6M"]
        data["P_Adjusted_Amount_6-12M"] = data["6-12M"] * data["Ref_ADF_NSFR.Indicator_ADF_6-12M"]
        data["P_Adjusted_Amount_>1Y"] = data[">1Y"] * data["Ref_ADF_NSFR.Indicator_ADF_>1Y"]

        # Colonnes à supprimer
        columns_to_drop = [
            "Ref_ADF_NSFR.D_ru",
            "Ref_ADF_NSFR.D_ac",
            "Ref_ADF_NSFR.Indicator_Ligne",
            "Ref_ADF_NSFR.Indicator_ADF",
        ]
        for col in columns_to_drop:
            if col in data.columns:
                data = data.drop(columns=col)

        return data

    
    def save_excel_with_structure(
        self,
        processed_data: dict,  # Clé : devise, Valeur : DataFrame
        excel_file_path: str,
        entity_list: list,
        run_timestamp: str,
        export_type: str,
        base_output_dir: str = "output",
        entity: str = None,  # Spécifique pour GRAN
        currency: str = "ALL"  # Spécifique pour GRAN
    ):
        """
        Sauvegarde les fichiers Excel selon une structure hiérarchique.

        :param processed_data: Données traitées (dict avec clés comme les devises et valeurs comme DataFrames).
        :param excel_file_path: Chemin du fichier Excel de base.
        :param entity_list: Liste des noms d'entités à filtrer (utilisé pour BILAN, CONSO, ALL).
        :param run_timestamp: Timestamp du traitement.
        :param export_type: Type d'export (ALL, BILAN, CONSO, GRAN).
        :param base_output_dir: Répertoire de sortie.
        :param entity: Nom de l'entité (spécifique pour GRAN).
        :param currency: Devise (spécifique pour GRAN).
        """
        base_folder = os.path.join(base_output_dir, f"RUN_{run_timestamp}_{export_type}")
        os.makedirs(base_folder, exist_ok=True)

        # Traitement pour BILAN et CONSO
        if export_type in ["BILAN", "CONSO"]:
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                currency_folder = os.path.join(base_folder, f"{export_type}_{currency}")
                os.makedirs(currency_folder, exist_ok=True)

                all_entities_folder = os.path.join(currency_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                by_entity_folder = os.path.join(currency_folder, "Reports_by_entity")
                os.makedirs(by_entity_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"NSFR_{export_type}_{currency}_All_Entities.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

                # Sauvegarder par entité
                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    entity_folder = os.path.join(by_entity_folder, entity)
                    os.makedirs(entity_folder, exist_ok=True)

                    if not entity_data.empty:
                        entity_file = os.path.join(entity_folder, f"NSFR_{export_type}_{currency}_{entity}.xlsx")
                        self.save_to_excel(entity_data, excel_file_path, entity_file)
                        print(f"Fichier sauvegardé : {entity_file}")

        # Traitement pour ALL
        elif export_type == "ALL":
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                all_entities_folder = os.path.join(base_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"NSFR_ALL_All_Entities_{currency}.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

    def save_to_excel(self, data: pd.DataFrame, template_path: str, output_path: str):
        """
        Sauvegarde des données dans un fichier Excel en utilisant un fichier template pour conserver la structure.

        :param data: DataFrame contenant les données à sauvegarder.
        :param template_path: Chemin du fichier Excel à utiliser comme template.
        :param output_path: Chemin du fichier Excel de sortie.
        """
        # Charger le classeur Excel existant
        workbook = load_workbook(template_path)
        first_sheet_name = workbook.sheetnames[0]  # Récupérer le nom de la première feuille
        first_sheet = workbook[first_sheet_name]  # Charger la première feuille uniquement

        # Effacer les anciennes données dans la première feuille
        for row in first_sheet.iter_rows():
            for cell in row:
                cell.value = None

        # Insérer les nouvelles données dans la première feuille
        for i, col_name in enumerate(data.columns, start=1):  # Parcourir les colonnes
            first_sheet.cell(row=1, column=i, value=col_name)  # Ajouter les noms de colonnes
            for j, value in enumerate(data[col_name], start=2):  # Parcourir les valeurs des colonnes
                first_sheet.cell(row=j, column=i, value=value)

        # Sauvegarder le fichier Excel avec les modifications
        workbook.save(output_path)
        print(f"Fichier sauvegardé : {output_path}")

        

        `,
                'AER.py' : `
import os
import pandas as pd
from openpyxl import load_workbook
from datetime import datetime 

class AER:
    def __init__(self, data_import: pd.DataFrame, ref_entite_path: str, ref_transfo_path: str, ref_aer_path: str, ref_adf_aer_path: str, run_timestamp: str,export_type: str):

        self.data = data_import

        # Charger et prétraiter les fichiers de référence
        self.ref_entite = self.preprocess_ref_entite(ref_entite_path)
        self.ref_transfo = self.preprocess_ref_transfo(ref_transfo_path)
        self.ref_aer = self.preprocess_ref_aer(ref_aer_path)
        self.ref_adf_aer = self.preprocess_ref_adf_aer(ref_adf_aer_path)
        self.run_timestamp = run_timestamp
        self.export_type = export_type
    
    @staticmethod
    def preprocess_ref_entite(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_Entite.xlsx :
        - Supprime les lignes ayant une valeur nulle dans la colonne 'd_ru'.
        - Ajoute le préfixe 'Ref_Entite.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['d_ru'])  # Supprime les lignes où 'd_ru' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Entite.'
        df = df.rename(columns=lambda col: f"Ref_Entite.{col}")
        return df

    @staticmethod
    def preprocess_ref_transfo(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        df['Transfo_aggregate_L1'] = df['Transfo_aggregate_L1'].astype(str)  # Convertit en texte
        df = df.drop_duplicates(subset=['Transfo_aggregate_L1'])  # Supprime les doublons

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Transfo_L1.'
        df = df.rename(columns=lambda col: f"Ref_Transfo_L1.{col}")
        return df
    
    @staticmethod
    def preprocess_ref_aer(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        if "Ligne_AER" in df.columns:
            df["Ligne_AER"] = df["Ligne_AER"].astype(str)
        df = df.rename(columns=lambda col: f"Ref_AER.{col}")
        return df

    @staticmethod
    def preprocess_ref_adf_aer(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        column_types = {
            "D_ru": "string",
            "Entité": "string",
            "D_ac": "string",
            "Indicator_Ligne": "string",
            "Indicator_ADF": "Int64",
        }
        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    if dtype == "Int64":
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")
        df = df.rename(columns=lambda col: f"Ref_ADF_AER.{col}")
        return df
    
    def filter_and_join_ref_entite(self,preprocessed_data):

        # 2.2. Filtrer les données
        filtered_data = preprocessed_data[
            (preprocessed_data["D_FL"] != "T99") & (preprocessed_data["D_ZONE"].notna())
        ]
        
        # 2.3. Joindre la table principale filtrée avec Ref_Entite
        joined_data = pd.merge(
            filtered_data,  # Table principale filtrée
            self.ref_entite,  # Table secondaire Ref_Entite
            left_on="D_RU",  # Colonne de jointure dans la table principale
            right_on="Ref_Entite.d_ru",  # Colonne de jointure dans la table secondaire
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data
    
    def join_with_ref_transfo(self, filtered_data: pd.DataFrame):
        
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale (déjà filtrée et jointe avec Ref_Entite)
            self.ref_transfo,  # Référence Ref_Transfo_L1 (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_Transfo_L1.Transfo_aggregate_L1",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Transfo_aggregate_L1 n'est pas null
        filtered_joined_data = joined_data[joined_data["Ref_Transfo_L1.Transfo_aggregate_L1"].notna()]

        # Retourner les données après jointure et filtrage
        return filtered_joined_data
    
    def join_with_ref_aer(self, data: pd.DataFrame) -> pd.DataFrame:
        # Effectuer la jointure externe gauche
        joined_data = pd.merge(
            data,  # Données principales
            self.ref_aer,  # Référence Ref_AER prétraitée
            left_on="D_AC",  # Colonne de jointure dans la table principale
            right_on="Ref_AER.Compte Transfo",  # Colonne de jointure dans la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où "Ref_AER.Ligne_AER" n'est pas null
        filtered_data = joined_data[joined_data["Ref_AER.Ligne_AER"].notna()]

        # Retourner les données après la jointure et le filtrage
        return filtered_data

    def group_and_join_ref_adf_aer(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier les colonnes nécessaires pour le regroupement
        required_columns = ["Ref_Entite.entité", "D_AC", "Ref_AER.Ligne_AER", "P_AMOUNT"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Regrouper les données et calculer la somme de P_AMOUNT
        grouped_data = (
            data.groupby(["Ref_Entite.entité", "D_AC", "Ref_AER.Ligne_AER"], as_index=False)
            .agg(P_Amount=("P_AMOUNT", "sum"))
        )

        # Effectuer la jointure externe gauche avec Ref_ADF_AER
        joined_data = pd.merge(
            grouped_data,  # Données regroupées
            self.ref_adf_aer,  # Référence Ref_ADF_AER prétraitée
            left_on=["D_AC", "Ref_AER.Ligne_AER"],  # Colonnes de jointure dans la table principale
            right_on=["Ref_ADF_AER.D_ac", "Ref_ADF_AER.Indicator_Ligne"],  # Colonnes de jointure dans la référence
            how="left",  # Jointure externe gauche
        )
        
        return joined_data
    
    def add_adjusted_amount(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier que les colonnes nécessaires sont présentes
        required_columns = ["P_Amount", "Ref_ADF_AER.Indicator_ADF"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter la colonne calculée
        data["P_Adjusted_Amount"] = data["P_Amount"] * data["Ref_ADF_AER.Indicator_ADF"]
        
        columns_to_drop = [
            "Ref_ADF_AER.D_ru",
            "Ref_ADF_AER.D_ac",
        ]
        for col in columns_to_drop:
            if col in data.columns:
                data = data.drop(columns=col)

        return data

    def save_excel_with_structure(
        self,
        processed_data: dict,  # Clé : devise, Valeur : DataFrame
        excel_file_path: str,
        entity_list: list,
        run_timestamp: str,
        export_type: str,
        base_output_dir: str = "output",
        entity: str = None,  # Spécifique pour GRAN
        currency: str = "ALL"  # Spécifique pour GRAN
    ):
        """
        Sauvegarde les fichiers Excel selon une structure hiérarchique.

        :param processed_data: Données traitées (dict avec clés comme les devises et valeurs comme DataFrames).
        :param excel_file_path: Chemin du fichier Excel de base.
        :param entity_list: Liste des noms d'entités à filtrer (utilisé pour BILAN, CONSO, ALL).
        :param run_timestamp: Timestamp du traitement.
        :param export_type: Type d'export (ALL, BILAN, CONSO, GRAN).
        :param base_output_dir: Répertoire de sortie.
        :param entity: Nom de l'entité (spécifique pour GRAN).
        :param currency: Devise (spécifique pour GRAN).
        """
        base_folder = os.path.join(base_output_dir, f"RUN_{run_timestamp}_{export_type}")
        os.makedirs(base_folder, exist_ok=True)

        # Traitement pour BILAN et CONSO
        if export_type in ["BILAN", "CONSO"]:
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                currency_folder = os.path.join(base_folder, f"{export_type}_{currency}")
                os.makedirs(currency_folder, exist_ok=True)

                all_entities_folder = os.path.join(currency_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                by_entity_folder = os.path.join(currency_folder, "Reports_by_entity")
                os.makedirs(by_entity_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"AER_{export_type}_{currency}_All_Entities.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

                # Sauvegarder par entité
                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    entity_folder = os.path.join(by_entity_folder, entity)
                    os.makedirs(entity_folder, exist_ok=True)

                    if not entity_data.empty:
                        entity_file = os.path.join(entity_folder, f"AER_{export_type}_{currency}_{entity}.xlsx")
                        self.save_to_excel(entity_data, excel_file_path, entity_file)
                        print(f"Fichier sauvegardé : {entity_file}")

        # Traitement pour ALL
        elif export_type == "ALL":
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                all_entities_folder = os.path.join(base_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)
                
                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"AER_ALL_All_Entities_{currency}.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)
                        
    def save_to_excel(self, data: pd.DataFrame, template_path: str, output_path: str):
        """
        Sauvegarde des données dans un fichier Excel en utilisant un fichier template pour conserver la structure.

        :param data: DataFrame contenant les données à sauvegarder.
        :param template_path: Chemin du fichier Excel à utiliser comme template.
        :param output_path: Chemin du fichier Excel de sortie.
        """
        # Charger le classeur Excel existant
        workbook = load_workbook(template_path)
        first_sheet_name = workbook.sheetnames[0]  # Récupérer le nom de la première feuille
        first_sheet = workbook[first_sheet_name]  # Charger la première feuille uniquement

        # Effacer les anciennes données dans la première feuille
        for row in first_sheet.iter_rows():
            for cell in row:
                cell.value = None

        # Insérer les nouvelles données dans la première feuille
        for i, col_name in enumerate(data.columns, start=1):  # Parcourir les colonnes
            first_sheet.cell(row=1, column=i, value=col_name)  # Ajouter les noms de colonnes
            for j, value in enumerate(data[col_name], start=2):  # Parcourir les valeurs des colonnes
                first_sheet.cell(row=j, column=i, value=value)

        # Sauvegarder le fichier Excel avec les modifications
        workbook.save(output_path)
        print(f"Fichier sauvegardé : {output_path}")
`,
                'QIS.py' : `
import os
import pandas as pd
from openpyxl import load_workbook
from datetime import datetime 

class QIS :
    def __init__(self, data_import: pd.DataFrame, ref_entite_path: str, ref_transfo_path: str, ref_qis_path: str, ref_adf_qis_path: str, ref_dzone_qis_path:str, run_timestamp: str, export_type : str):

        self.data = data_import

        # Charger et prétraiter les fichiers de référence
        self.ref_entite = self.preprocess_ref_entite(ref_entite_path)
        self.ref_transfo = self.preprocess_ref_transfo(ref_transfo_path)
        self.ref_qis = self.preprocess_ref_qis(ref_qis_path)
        self.ref_adf_qis = self.preprocess_ref_adf_qis(ref_adf_qis_path)
        self.ref_dzone_qis = self.preprocess_ref_dzone_qis(ref_dzone_qis_path)
        self.run_timestamp = run_timestamp
        self.export_type = export_type
        
    
    @staticmethod
    def preprocess_ref_entite(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_Entite.xlsx :
        - Supprime les lignes ayant une valeur nulle dans la colonne 'd_ru'.
        - Ajoute le préfixe 'Ref_Entite.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['d_ru'])  # Supprime les lignes où 'd_ru' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Entite.'
        df = df.rename(columns=lambda col: f"Ref_Entite.{col}")
        return df

    @staticmethod
    def preprocess_ref_transfo(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        df['Transfo_aggregate_L1'] = df['Transfo_aggregate_L1'].astype(str)  # Convertit en texte
        df = df.drop_duplicates(subset=['Transfo_aggregate_L1'])  # Supprime les doublons

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Transfo_L1.'
        df = df.rename(columns=lambda col: f"Ref_Transfo_L1.{col}")
        return df
    
    @staticmethod
    def preprocess_ref_qis(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_QIS.xlsx :
        - Change les types des colonnes.
        - Ajoute le préfixe 'Ref_QIS.' à toutes les colonnes.
        """
        df = pd.read_excel(file_path)

        # Vérification des colonnes nécessaires
        df = df.rename(columns={'Compte transfo':'Compte Transfo'})
        required_columns = ["Compte Transfo", "Ligne_QIS"]
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le fichier Ref_QIS.")

        # Changer les types des colonnes
        column_types = {
            "Compte Transfo": "string",
            "Ligne_QIS": "string",
            "PCT_QIS": "string",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne '{col}' en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe 'Ref_QIS.'
        df = df.rename(columns=lambda col: f"Ref_QIS.{col}")

        return df

    

    @staticmethod
    def preprocess_ref_adf_qis(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)

        column_types = {
            "D_ru": "string",
            "Entité": "string",
            "D_ac": "string",
            "Indicator_Ligne": "string",
            "Indicator_ADF": "Int64",
            "Indicator_ADF_0-6M": "float64",
            "Indicator_ADF_6-12M": "float64",
            "Indicator_ADF_>1Y": "float64",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    if dtype == "float64":
                        # Remplacer les valeurs non numériques
                        df[col] = pd.to_numeric(df[col].replace("NOT APPLICABLE", None), errors='coerce')
                    elif dtype == "Int64":
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe Ref_ADF_NSFR.
        df = df.rename(columns=lambda x: f"Ref_ADF_NSFR.{x}")

        return df

    @staticmethod
    def preprocess_ref_dzone_qis(file_path: str) -> pd.DataFrame:

        # Charger le fichier
        df = pd.read_excel(file_path)

        # Changer les types des colonnes
        column_types = {
            "Colonne1": "string",
            "Colonne2": "string",
        }
        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes
        rename_columns = {
            "Colonne1": "D_ZONE",
            "Colonne2": "NSFR_Bucket",
        }
        df = df.rename(columns=rename_columns)

        # Ignorer la première ligne
        df = df.iloc[1:].reset_index(drop=True)

        # Ajouter le préfixe 'Ref_DZONE_NSFR.' aux colonnes
        df = df.rename(columns=lambda col: f"Ref_DZONE_NSFR.{col}")

        return df

    def filter_and_join_ref_entite(self,preprocessed_data):

        # 2.2. Filtrer les données
        filtered_data = preprocessed_data[
            (preprocessed_data["D_FL"] != "T99") & (preprocessed_data["D_ZONE"].notna())
        ]
        
        # 2.3. Joindre la table principale filtrée avec Ref_Entite
        joined_data = pd.merge(
            filtered_data,  # Table principale filtrée
            self.ref_entite,  # Table secondaire Ref_Entite
            left_on="D_RU",  # Colonne de jointure dans la table principale
            right_on="Ref_Entite.d_ru",  # Colonne de jointure dans la table secondaire
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data
    
    def join_with_ref_qis(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in filtered_data.columns:
            raise ValueError("La colonne 'D_AC' est manquante dans le DataFrame principal.")
        if "Ref_QIS.Compte Transfo" not in self.ref_qis.columns:
            raise ValueError("La colonne 'Ref_QIS.Compte Transfo' est manquante dans la table Ref_QIS.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_qis,  # Référence Ref_QIS
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_QIS.Compte Transfo",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Ref_QIS.Ligne_QIS n'est pas null
        if "Ref_QIS.Ligne_QIS" not in joined_data.columns:
            raise ValueError("La colonne 'Ref_QIS.Ligne_QIS' est manquante dans le DataFrame après jointure.")
        filtered_joined_data = joined_data[joined_data["Ref_QIS.Ligne_QIS"].notna()]

        return filtered_joined_data

    def join_with_ref_transfo(self, filtered_data: pd.DataFrame):
    
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale (déjà filtrée et jointe avec Ref_Entite)
            self.ref_transfo,  # Référence Ref_Transfo_L1 (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_Transfo_L1.Transfo_aggregate_L1",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Transfo_aggregate_L1 n'est pas null
        filtered_joined_data = joined_data[joined_data["Ref_Transfo_L1.Transfo_aggregate_L1"].notna()]

        # Retourner les données après jointure et filtrage
        return filtered_joined_data

    
    def join_with_ref_dzone_qis(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_ZONE" not in filtered_data.columns:
            raise ValueError("La colonne 'D_ZONE' est manquante dans le DataFrame principal.")
        if "Ref_DZONE_NSFR.D_ZONE" not in self.ref_dzone_qis.columns:
            raise ValueError("La colonne 'Ref_DZONE_NSFR.D_ZONE' est manquante dans la table Ref_DZONE_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_dzone_qis,  # Référence Ref_DZONE_NSFR
            left_on="D_ZONE",  # Colonne de la table principale
            right_on="Ref_DZONE_NSFR.D_ZONE",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    
    
    def group_and_sum_unadjusted_p_amount(self, data: pd.DataFrame) -> pd.DataFrame:
    
        # Colonnes utilisées pour le regroupement
        group_columns = [
            "Ref_Entite.entité", 
            "D_AC", 
            "Ref_DZONE_NSFR.NSFR_Bucket", 
            "Ref_QIS.Ligne_QIS"
        ]

        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = group_columns + ["P_AMOUNT"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Regrouper les données et calculer la somme
        grouped_data = (
            data.groupby(group_columns, as_index=False)
            .agg(Unadjusted_P_Amount=("P_AMOUNT", "sum"))
        )

        return grouped_data

    def pivot_and_reorder(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_QIS.Ligne_QIS",
            "Ref_DZONE_NSFR.NSFR_Bucket",
            "Unadjusted_P_Amount",
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Pivoter les données
        pivoted_data = data.pivot_table(
            index=["Ref_Entite.entité", "D_AC", "Ref_QIS.Ligne_QIS"],  # Colonnes fixes
            columns="Ref_DZONE_NSFR.NSFR_Bucket",  # Colonne à pivoter
            values="Unadjusted_P_Amount",  # Valeur à agréger
            aggfunc="sum",  # Fonction d'agrégation
            fill_value=0,  # Remplir les valeurs manquantes par 0
        ).reset_index()

        # Réorganiser les colonnes
        desired_order = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_QIS.Ligne_QIS",
            "0-6M",
            "6-12M",
            ">1Y",
        ]

        # Ajouter les colonnes manquantes avec des zéros
        for col in desired_order:
            if col not in pivoted_data.columns:
                print(f"Ajout de la colonne manquante '{col}' avec des valeurs 0.")
                pivoted_data[col] = 0

        # Réorganiser les colonnes dans l'ordre souhaité
        pivoted_data = pivoted_data[desired_order]

        return pivoted_data


    
    def join_with_ref_adf_qis(self, data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in data.columns or "Ref_QIS.Ligne_QIS" not in data.columns:
            raise ValueError("Les colonnes 'D_AC' ou 'Ref_QIS.Ligne_QIS' sont manquantes dans la table principale.")
        if "Ref_ADF_NSFR.D_ac" not in self.ref_adf_qis.columns or "Ref_ADF_NSFR.Indicator_Ligne" not in self.ref_adf_qis.columns:
            raise ValueError("Les colonnes 'Ref_ADF_NSFR.D_ac' ou 'Ref_ADF_NSFR.Indicator_Ligne' sont manquantes dans la table Ref_ADF_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            data,  # Table principale
            self.ref_adf_qis,  # Référence Ref_ADF_NSFR
            left_on=["D_AC", "Ref_QIS.Ligne_QIS"],  # Colonnes de la table principale
            right_on=["Ref_ADF_NSFR.D_ac", "Ref_ADF_NSFR.Indicator_Ligne"],  # Colonnes de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    def add_adjusted_amounts(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "0-6M", 
            "6-12M", 
            ">1Y", 
            "Ref_ADF_NSFR.Indicator_ADF_0-6M", 
            "Ref_ADF_NSFR.Indicator_ADF_6-12M", 
            "Ref_ADF_NSFR.Indicator_ADF_>1Y"
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter les colonnes calculées
        data["P_Adjusted_Amount_0-6M"] = data["0-6M"] * data["Ref_ADF_NSFR.Indicator_ADF_0-6M"]
        data["P_Adjusted_Amount_6-12M"] = data["6-12M"] * data["Ref_ADF_NSFR.Indicator_ADF_6-12M"]
        data["P_Adjusted_Amount_>1Y"] = data[">1Y"] * data["Ref_ADF_NSFR.Indicator_ADF_>1Y"]

        # Colonnes à supprimer
        columns_to_drop = [
            "Ref_ADF_NSFR.D_ru",
            "Ref_ADF_NSFR.D_ac",
            "Ref_ADF_NSFR.Indicator_Ligne",
            "Ref_ADF_NSFR.Indicator_ADF",
        ]
        for col in columns_to_drop:
            if col in data.columns:
                data = data.drop(columns=col)

        return data

    
    def save_excel_with_structure(
        self,
        processed_data: dict,  # Clé : devise, Valeur : DataFrame
        excel_file_path: str,
        entity_list: list,
        run_timestamp: str,
        export_type: str,
        base_output_dir: str = "output",
        entity: str = None,  # Spécifique pour GRAN
        currency: str = "ALL"  # Spécifique pour GRAN
    ):
        """
        Sauvegarde les fichiers Excel selon une structure hiérarchique.

        :param processed_data: Données traitées (dict avec clés comme les devises et valeurs comme DataFrames).
        :param excel_file_path: Chemin du fichier Excel de base.
        :param entity_list: Liste des noms d'entités à filtrer (utilisé pour BILAN, CONSO, ALL).
        :param run_timestamp: Timestamp du traitement.
        :param export_type: Type d'export (ALL, BILAN, CONSO, GRAN).
        :param base_output_dir: Répertoire de sortie.
        :param entity: Nom de l'entité (spécifique pour GRAN).
        :param currency: Devise (spécifique pour GRAN).
        """
        base_folder = os.path.join(base_output_dir, f"RUN_{run_timestamp}_{export_type}")
        os.makedirs(base_folder, exist_ok=True)

        # Traitement pour BILAN et CONSO
        if export_type in ["BILAN", "CONSO"]:
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                currency_folder = os.path.join(base_folder, f"{export_type}_{currency}")
                os.makedirs(currency_folder, exist_ok=True)

                all_entities_folder = os.path.join(currency_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                by_entity_folder = os.path.join(currency_folder, "Reports_by_entity")
                os.makedirs(by_entity_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"QIS_{export_type}_{currency}_All_Entities.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

                # Sauvegarder par entité
                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    entity_folder = os.path.join(by_entity_folder, entity)
                    os.makedirs(entity_folder, exist_ok=True)

                    if not entity_data.empty:
                        entity_file = os.path.join(entity_folder, f"QIS_{export_type}_{currency}_{entity}.xlsx")
                        self.save_to_excel(entity_data, excel_file_path, entity_file)
                        print(f"Fichier sauvegardé : {entity_file}")

        # Traitement pour ALL
        elif export_type == "ALL":
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                all_entities_folder = os.path.join(base_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"QIS_ALL_All_Entities_{currency}.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

    def save_to_excel(self, data: pd.DataFrame, template_path: str, output_path: str):
        """
        Sauvegarde des données dans un fichier Excel en utilisant un fichier template pour conserver la structure.

        :param data: DataFrame contenant les données à sauvegarder.
        :param template_path: Chemin du fichier Excel à utiliser comme template.
        :param output_path: Chemin du fichier Excel de sortie.
        """
        # Charger le classeur Excel existant
        workbook = load_workbook(template_path)
        first_sheet_name = workbook.sheetnames[0]  # Récupérer le nom de la première feuille
        first_sheet = workbook[first_sheet_name]  # Charger la première feuille uniquement

        # Effacer les anciennes données dans la première feuille
        for row in first_sheet.iter_rows():
            for cell in row:
                cell.value = None

        # Insérer les nouvelles données dans la première feuille
        for i, col_name in enumerate(data.columns, start=1):  # Parcourir les colonnes
            first_sheet.cell(row=1, column=i, value=col_name)  # Ajouter les noms de colonnes
            for j, value in enumerate(data[col_name], start=2):  # Parcourir les valeurs des colonnes
                first_sheet.cell(row=j, column=i, value=value)

        # Sauvegarder le fichier Excel avec les modifications
        workbook.save(output_path)
        print(f"Fichier sauvegardé : {output_path}")
`,
                'ALMM.py' : `
import os
import pandas as pd
from openpyxl import load_workbook
from datetime import datetime 

class ALMM :
    def __init__(self, data_import: pd.DataFrame, ref_entite_path: str, ref_transfo_path: str, ref_almm_path: str, ref_adf_almm_path: str, ref_dzone_almm_path:str, run_timestamp: str, export_type : str):

        self.data = data_import

        # Charger et prétraiter les fichiers de référence
        self.ref_entite = self.preprocess_ref_entite(ref_entite_path)
        self.ref_transfo = self.preprocess_ref_transfo(ref_transfo_path)
        self.ref_almm = self.preprocess_ref_almm(ref_almm_path)
        self.ref_adf_almm = self.preprocess_ref_adf_almm(ref_adf_almm_path)
        self.ref_dzone_almm = self.preprocess_ref_dzone_almm(ref_dzone_almm_path)
        self.run_timestamp = run_timestamp
        self.export_type = export_type

    @staticmethod
    def preprocess_ref_entite(file_path: str) -> pd.DataFrame:
        """
        Prétraitement pour Ref_Entite.xlsx :
        - Supprime les lignes ayant une valeur nulle dans la colonne 'd_ru'.
        - Ajoute le préfixe 'Ref_Entite.' à tous les noms de colonnes.
        """
        df = pd.read_excel(file_path)
        df = df.dropna(subset=['d_ru'])  # Supprime les lignes où 'd_ru' est null

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Entite.'
        df = df.rename(columns=lambda col: f"Ref_Entite.{col}")
        return df

    @staticmethod
    def preprocess_ref_transfo(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)
        df['Transfo_aggregate_L1'] = df['Transfo_aggregate_L1'].astype(str)  # Convertit en texte
        df = df.drop_duplicates(subset=['Transfo_aggregate_L1'])  # Supprime les doublons

        # Renommer les colonnes en ajoutant le préfixe 'Ref_Transfo_L1.'
        df = df.rename(columns=lambda col: f"Ref_Transfo_L1.{col}")
        return df
    
    @staticmethod
    def preprocess_ref_almm(file_path: str) -> pd.DataFrame:

        df = pd.read_excel(file_path)

        # Changer les types des colonnes
        column_types = {
            "Compte Transfo": "string",
            "Ligne_NSFR": "string",
            "PCT_NSFR": "string",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe 'Ref_NSFR.'
        df = df.rename(columns=lambda col: f"Ref_NSFR.{col}")

        return df

    @staticmethod
    def preprocess_ref_adf_almm(file_path: str) -> pd.DataFrame:
        df = pd.read_excel(file_path)

        column_types = {
            "D_ru": "string",
            "Entité": "string",
            "D_ac": "string",
            "Indicator_Ligne": "string",
            "Indicator_ADF": "Int64",
            "Indicator_ADF_0-6M": "float64",
            "Indicator_ADF_6-12M": "float64",
            "Indicator_ADF_>1Y": "float64",
        }

        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    if dtype == "float64":
                        # Remplacer les valeurs non numériques
                        df[col] = pd.to_numeric(df[col].replace("NOT APPLICABLE", None), errors='coerce')
                    elif dtype == "Int64":
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")
                    else:
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes avec le préfixe Ref_ADF_NSFR.
        df = df.rename(columns=lambda x: f"Ref_ADF_NSFR.{x}")

        return df

    @staticmethod
    def preprocess_ref_dzone_almm(file_path: str) -> pd.DataFrame:

        # Charger le fichier
        df = pd.read_excel(file_path)

        # Changer les types des colonnes
        column_types = {
            "Colonne1": "string",
            "Colonne2": "string",
        }
        for col, dtype in column_types.items():
            if col in df.columns:
                try:
                    df[col] = df[col].astype(dtype)
                except Exception as e:
                    print(f"Erreur lors de la conversion de la colonne {col} en {dtype}: {e}")

        # Renommer les colonnes
        rename_columns = {
            "Colonne1": "D_ZONE",
            "Colonne2": "NSFR_Bucket",
        }
        df = df.rename(columns=rename_columns)

        # Ignorer la première ligne
        df = df.iloc[1:].reset_index(drop=True)

        # Ajouter le préfixe 'Ref_DZONE_NSFR.' aux colonnes
        df = df.rename(columns=lambda col: f"Ref_DZONE_NSFR.{col}")

        return df

    def filter_and_join_ref_entite(self,preprocessed_data):

        # 2.2. Filtrer les données
        filtered_data = preprocessed_data[
            (preprocessed_data["D_FL"] != "T99") & (preprocessed_data["D_ZONE"].notna())
        ]
        
        # 2.3. Joindre la table principale filtrée avec Ref_Entite
        joined_data = pd.merge(
            filtered_data,  # Table principale filtrée
            self.ref_entite,  # Table secondaire Ref_Entite
            left_on="D_RU",  # Colonne de jointure dans la table principale
            right_on="Ref_Entite.d_ru",  # Colonne de jointure dans la table secondaire
            how="left",  # Jointure externe gauche
        )

        # Retourner les données après jointure
        return joined_data

    def join_with_ref_transfo(self, filtered_data: pd.DataFrame):
    
        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale (déjà filtrée et jointe avec Ref_Entite)
            self.ref_transfo,  # Référence Ref_Transfo_L1 (prétraitée dynamiquement)
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_Transfo_L1.Transfo_aggregate_L1",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Transfo_aggregate_L1 n'est pas null
        filtered_joined_data = joined_data[joined_data["Ref_Transfo_L1.Transfo_aggregate_L1"].notna()]

        # Retourner les données après jointure et filtrage
        return filtered_joined_data

    
    def join_with_ref_dzone_almm(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_ZONE" not in filtered_data.columns:
            raise ValueError("La colonne 'D_ZONE' est manquante dans le DataFrame principal.")
        if "Ref_DZONE_NSFR.D_ZONE" not in self.ref_dzone_almm.columns:
            raise ValueError("La colonne 'Ref_DZONE_NSFR.D_ZONE' est manquante dans la table Ref_DZONE_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_dzone_almm,  # Référence Ref_DZONE_NSFR
            left_on="D_ZONE",  # Colonne de la table principale
            right_on="Ref_DZONE_NSFR.D_ZONE",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    def join_with_ref_almm(self, filtered_data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in filtered_data.columns:
            raise ValueError("La colonne 'D_AC' est manquante dans le DataFrame principal.")
        if "Ref_NSFR.Compte Transfo" not in self.ref_almm.columns:
            raise ValueError("La colonne 'Ref_NSFR.Compte Transfo' est manquante dans la table Ref_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            filtered_data,  # Table principale
            self.ref_almm,  # Référence Ref_NSFR
            left_on="D_AC",  # Colonne de la table principale
            right_on="Ref_NSFR.Compte Transfo",  # Colonne de la référence
            how="left",  # Jointure externe gauche
        )

        # Filtrer les lignes où Ref_NSFR.Ligne_NSFR n'est pas null
        if "Ref_NSFR.Ligne_NSFR" not in joined_data.columns:
            raise ValueError("La colonne 'Ref_NSFR.Ligne_NSFR' est manquante dans le DataFrame après jointure.")
        filtered_joined_data = joined_data[joined_data["Ref_NSFR.Ligne_NSFR"].notna()]

        return filtered_joined_data
    
    def group_and_sum_unadjusted_p_amount(self, data: pd.DataFrame) -> pd.DataFrame:
    
        # Colonnes utilisées pour le regroupement
        group_columns = [
            "Ref_Entite.entité", 
            "D_AC", 
            "Ref_DZONE_NSFR.NSFR_Bucket", 
            "Ref_NSFR.Ligne_NSFR"
        ]

        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = group_columns + ["P_AMOUNT"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Regrouper les données et calculer la somme
        grouped_data = (
            data.groupby(group_columns, as_index=False)
            .agg(Unadjusted_P_Amount=("P_AMOUNT", "sum"))
        )

        return grouped_data

    def pivot_and_reorder(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_NSFR.Ligne_NSFR",
            "Ref_DZONE_NSFR.NSFR_Bucket",
            "Unadjusted_P_Amount",
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Pivoter les données
        pivoted_data = data.pivot_table(
            index=["Ref_Entite.entité", "D_AC", "Ref_NSFR.Ligne_NSFR"],  # Colonnes fixes
            columns="Ref_DZONE_NSFR.NSFR_Bucket",  # Colonne à pivoter
            values="Unadjusted_P_Amount",  # Valeur à agréger
            aggfunc="sum",  # Fonction d'agrégation
            fill_value=0,  # Remplir les valeurs manquantes par 0
        ).reset_index()

        # Réorganiser les colonnes
        desired_order = [
            "Ref_Entite.entité",
            "D_AC",
            "Ref_NSFR.Ligne_NSFR",
            "0-6M",
            "6-12M",
            ">1Y",
        ]
        for col in desired_order:
            if col not in pivoted_data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame pivoté.")

        reordered_data = pivoted_data[desired_order]

        return reordered_data

    
    def join_with_ref_adf_almm(self, data: pd.DataFrame) -> pd.DataFrame:

        # Vérifier que les colonnes nécessaires sont présentes
        if "D_AC" not in data.columns or "Ref_NSFR.Ligne_NSFR" not in data.columns:
            raise ValueError("Les colonnes 'D_AC' ou 'Ref_NSFR.Ligne_NSFR' sont manquantes dans la table principale.")
        if "Ref_ADF_NSFR.D_ac" not in self.ref_adf_almm.columns or "Ref_ADF_NSFR.Indicator_Ligne" not in self.ref_adf_almm.columns:
            raise ValueError("Les colonnes 'Ref_ADF_NSFR.D_ac' ou 'Ref_ADF_NSFR.Indicator_Ligne' sont manquantes dans la table Ref_ADF_NSFR.")

        # Effectuer la jointure
        joined_data = pd.merge(
            data,  # Table principale
            self.ref_adf_almm,  # Référence Ref_ADF_NSFR
            left_on=["D_AC", "Ref_NSFR.Ligne_NSFR"],  # Colonnes de la table principale
            right_on=["Ref_ADF_NSFR.D_ac", "Ref_ADF_NSFR.Indicator_Ligne"],  # Colonnes de la référence
            how="left",  # Jointure externe gauche
        )

        return joined_data

    def add_adjusted_amounts(self, data: pd.DataFrame) -> pd.DataFrame:
        # Vérifier que toutes les colonnes nécessaires sont présentes
        required_columns = [
            "0-6M", 
            "6-12M", 
            ">1Y", 
            "Ref_ADF_NSFR.Indicator_ADF_0-6M", 
            "Ref_ADF_NSFR.Indicator_ADF_6-12M", 
            "Ref_ADF_NSFR.Indicator_ADF_>1Y"
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"La colonne '{col}' est manquante dans le DataFrame.")

        # Ajouter les colonnes calculées
        data["P_Adjusted_Amount_0-6M"] = data["0-6M"] * data["Ref_ADF_NSFR.Indicator_ADF_0-6M"]
        data["P_Adjusted_Amount_6-12M"] = data["6-12M"] * data["Ref_ADF_NSFR.Indicator_ADF_6-12M"]
        data["P_Adjusted_Amount_>1Y"] = data[">1Y"] * data["Ref_ADF_NSFR.Indicator_ADF_>1Y"]

        # Colonnes à supprimer
        columns_to_drop = [
            "Ref_ADF_NSFR.D_ru",
            "Ref_ADF_NSFR.D_ac",
            "Ref_ADF_NSFR.Indicator_Ligne",
            "Ref_ADF_NSFR.Indicator_ADF",
        ]
        for col in columns_to_drop:
            if col in data.columns:
                data = data.drop(columns=col)

        return data

    
    def save_excel_with_structure(
        self,
        processed_data: dict,  # Clé : devise, Valeur : DataFrame
        excel_file_path: str,
        entity_list: list,
        run_timestamp: str,
        export_type: str,
        base_output_dir: str = "output",
        entity: str = None,  # Spécifique pour GRAN
        currency: str = "ALL"  # Spécifique pour GRAN
    ):
        """
        Sauvegarde les fichiers Excel selon une structure hiérarchique.

        :param processed_data: Données traitées (dict avec clés comme les devises et valeurs comme DataFrames).
        :param excel_file_path: Chemin du fichier Excel de base.
        :param entity_list: Liste des noms d'entités à filtrer (utilisé pour BILAN, CONSO, ALL).
        :param run_timestamp: Timestamp du traitement.
        :param export_type: Type d'export (ALL, BILAN, CONSO, GRAN).
        :param base_output_dir: Répertoire de sortie.
        :param entity: Nom de l'entité (spécifique pour GRAN).
        :param currency: Devise (spécifique pour GRAN).
        """
        base_folder = os.path.join(base_output_dir, f"RUN_{run_timestamp}_{export_type}")
        os.makedirs(base_folder, exist_ok=True)

        # Traitement pour BILAN et CONSO
        if export_type in ["BILAN", "CONSO"]:
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                currency_folder = os.path.join(base_folder, f"{export_type}_{currency}")
                os.makedirs(currency_folder, exist_ok=True)

                all_entities_folder = os.path.join(currency_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                by_entity_folder = os.path.join(currency_folder, "Reports_by_entity")
                os.makedirs(by_entity_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"ALMM_{export_type}_{currency}_All_Entities.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

                # Sauvegarder par entité
                for entity in entity_list:
                    entity_data = data[data["Ref_Entite.entité"] == entity]
                    entity_folder = os.path.join(by_entity_folder, entity)
                    os.makedirs(entity_folder, exist_ok=True)

                    if not entity_data.empty:
                        entity_file = os.path.join(entity_folder, f"ALMM_{export_type}_{currency}_{entity}.xlsx")
                        self.save_to_excel(entity_data, excel_file_path, entity_file)
                        print(f"Fichier sauvegardé : {entity_file}")

        # Traitement pour ALL
        elif export_type == "ALL":
            for currency, data in processed_data.items():
                if not isinstance(data, pd.DataFrame):
                    print(f"Les données pour la devise '{currency}' ne sont pas un DataFrame. Traitement ignoré.")
                    continue

                all_entities_folder = os.path.join(base_folder, "Reports_all_entities")
                os.makedirs(all_entities_folder, exist_ok=True)

                # Sauvegarder les fichiers globaux
                global_file = os.path.join(all_entities_folder, f"ALMM_ALL_All_Entities_{currency}.xlsx")
                self.save_to_excel(data, excel_file_path, global_file)

    def save_to_excel(self, data: pd.DataFrame, template_path: str, output_path: str):
        """
        Sauvegarde des données dans un fichier Excel en utilisant un fichier template pour conserver la structure.

        :param data: DataFrame contenant les données à sauvegarder.
        :param template_path: Chemin du fichier Excel à utiliser comme template.
        :param output_path: Chemin du fichier Excel de sortie.
        """
        # Charger le classeur Excel existant
        workbook = load_workbook(template_path)
        first_sheet_name = workbook.sheetnames[0]  # Récupérer le nom de la première feuille
        first_sheet = workbook[first_sheet_name]  # Charger la première feuille uniquement

        # Effacer les anciennes données dans la première feuille
        for row in first_sheet.iter_rows():
            for cell in row:
                cell.value = None

        # Insérer les nouvelles données dans la première feuille
        for i, col_name in enumerate(data.columns, start=1):  # Parcourir les colonnes
            first_sheet.cell(row=1, column=i, value=col_name)  # Ajouter les noms de colonnes
            for j, value in enumerate(data[col_name], start=2):  # Parcourir les valeurs des colonnes
                first_sheet.cell(row=j, column=i, value=value)

        # Sauvegarder le fichier Excel avec les modifications
        workbook.save(output_path)
        print(f"Fichier sauvegardé : {output_path}")

                `,
                },
            requirements: ["pandas", "openpyxl", "streamlit", "xlsxwriter"],
            },
           document.getElementById("root"));
       </script>
   </body>
   </html>
